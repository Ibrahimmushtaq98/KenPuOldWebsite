<html>

    
<!-- Mirrored from db.science.uoit.ca/library/teaching/machine-learning by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 06 Apr 2020 22:40:16 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
        <meta name="viewport" content="width=device-width, initial-scale=1 maximum-scale=1">
        <link rel="icon" href="../static/favicon.png"/>
        <title>machine-learning</title>
        <!--
        <link rel="stylesheet" href="/library/static/yeti-bootstrap.min.css">
        -->
        <link rel="stylesheet" href="../static/font-awesome/css/font-awesome.min.css">
        <link rel="stylesheet" href="../static/css/fonts/fonts.css">
        <link rel=stylesheet href="../static/prism/prism.css">
        <link rel=stylesheet href="../static/client/css/app.css">
        <link rel=stylesheet href="../static/client/css/bookie.css">
    </head>

<body>
<div id="app"></div>
<div id="app-resources" style="display: none">
    <ul>
        
        <li><a href="index.clj">index.clj</a></li>
        
        <li><a href="regulation/regulated.html">regulation/regulated.png</a></li>
        
        <li><a href="regulation/certain-dirichlet.html">regulation/certain-dirichlet.png</a></li>
        
        <li><a href="regulation/no-dirichlet.html">regulation/no-dirichlet.png</a></li>
        
        <li><a href="regulation/uncertain-dirichlet.html">regulation/uncertain-dirichlet.png</a></li>
        
        <li><a href="regulation/unregulated.html">regulation/unregulated.png</a></li>
        
        <li><a href="regulation/index.html">regulation/index.clj</a></li>
        
        <li><a href="sequence-learning/seq2seq.html">sequence-learning/seq2seq.png</a></li>
        
        <li><a href="sequence-learning/lstm-i.html">sequence-learning/lstm-i.png</a></li>
        
        <li><a href="sequence-learning/lstm-f.html">sequence-learning/lstm-f.png</a></li>
        
        <li><a href="sequence-learning/lstm-o.html">sequence-learning/lstm-o.png</a></li>
        
        <li><a href="sequence-learning/rnn-unfolded.html">sequence-learning/rnn-unfolded.jpg</a></li>
        
        <li><a href="sequence-learning/index.html">sequence-learning/index.clj</a></li>
        
        <li><a href="sequence-learning/lstm-c-line.html">sequence-learning/lstm-c-line.png</a></li>
        
        <li><a href="sequence-learning/rnn-deps.html">sequence-learning/rnn-deps.png</a></li>
        
        <li><a href="sequence-learning/seq2seq-stateful.html">sequence-learning/seq2seq-stateful.png</a></li>
        
        <li><a href="tensorflow/tensorflow.html">tensorflow/tensorflow.html</a></li>
        
        <li><a href="tensorflow/tensorflow-2.html">tensorflow/tensorflow.ipynb</a></li>
        
        <li><a href="tensorflow/index.html">tensorflow/index.clj</a></li>
        
        <li><a href="tensors/index.html">tensors/index.clj</a></li>
        
        <li><a href="func-vector-space/index.html">func-vector-space/index.clj</a></li>
        
        <li><a href="func-vector-space/sigmoid.html">func-vector-space/sigmoid.png</a></li>
        
        <li><a href="vector-spaces/2019_09_15_linear_algebra.html">vector-spaces/2019_09_15_linear_algebra.html</a></li>
        
        <li><a href="vector-spaces/2019_09_15_linear_algebra-2.html">vector-spaces/2019_09_15_linear_algebra.ipynb</a></li>
        
        <li><a href="vector-spaces/index.html">vector-spaces/index.clj</a></li>
        
        <li><a href="embedding/index.html">embedding/index.clj</a></li>
        
        <li><a href="cross-validation/test-err-final.html">cross-validation/test-err-final.png</a></li>
        
        <li><a href="cross-validation/with-missing.html">cross-validation/with-missing.png</a></li>
        
        <li><a href="cross-validation/overfitted.html">cross-validation/overfitted.png</a></li>
        
        <li><a href="cross-validation/test-err.html">cross-validation/test-err.png</a></li>
        
        <li><a href="cross-validation/no-noise.html">cross-validation/no-noise.png</a></li>
        
        <li><a href="cross-validation/mlp.html">cross-validation/mlp.png</a></li>
        
        <li><a href="cross-validation/index.html">cross-validation/index.clj</a></li>
        
        <li><a href="cross-validation/regulation-learning.html">cross-validation/regulation-learning.png</a></li>
        
        <li><a href="cross-validation/with-noise.html">cross-validation/with-noise.png</a></li>
        
        <li><a href="cross-validation/overfitted-cost.html">cross-validation/overfitted-cost.png</a></li>
        
        <li><a href="calculus/vector-field-1.html">calculus/vector-field-1.gif</a></li>
        
        <li><a href="calculus/vector-field-2.html">calculus/vector-field-2.jpg</a></li>
        
        <li><a href="calculus/local-global-maxima.html">calculus/local-global-maxima.jpg</a></li>
        
        <li><a href="calculus/index.html">calculus/index.clj</a></li>
        
        <li><a href="kernel-methods/kernel-svm.html">kernel-methods/kernel-svm.png</a></li>
        
        <li><a href="kernel-methods/sep-surface.html">kernel-methods/sep-surface.png</a></li>
        
        <li><a href="kernel-methods/hyperplane.html">kernel-methods/hyperplane.png</a></li>
        
        <li><a href="kernel-methods/rbfn.html">kernel-methods/rbfn.png</a></li>
        
        <li><a href="kernel-methods/svm-margin.html">kernel-methods/svm-margin.png</a></li>
        
        <li><a href="kernel-methods/nosep.html">kernel-methods/nosep.png</a></li>
        
        <li><a href="kernel-methods/mapped.html">kernel-methods/mapped.png</a></li>
        
        <li><a href="kernel-methods/index.html">kernel-methods/index.clj</a></li>
        
        <li><a href="intro/index.html">intro/index.clj</a></li>
        
        <li><a href="neural-networks/feedforward.html">neural-networks/feedforward.jpg</a></li>
        
        <li><a href="neural-networks/tanh.html">neural-networks/tanh.svg</a></li>
        
        <li><a href="neural-networks/sigmoid.html">neural-networks/sigmoid.svg</a></li>
        
        <li><a href="neural-networks/relu.html">neural-networks/relu.svg</a></li>
        
        <li><a href="neural-networks/multi-feedforward.html">neural-networks/multi-feedforward.png</a></li>
        
        <li><a href="neural-networks/index.html">neural-networks/index.clj</a></li>
        
        <li><a href="neural-networks/mlp.html">neural-networks/mlp.jpg</a></li>
        
        <li><a href="representations/tensor_representation.html">representations/tensor_representation.ipynb</a></li>
        
        <li><a href="representations/tensor_representation-2.html">representations/tensor_representation.html</a></li>
        
        <li><a href="representations/index.html">representations/index.clj</a></li>
        
        <li><a href="convolutional-layers/neighbour-neuron.html">convolutional-layers/neighbour-neuron.jpg</a></li>
        
        <li><a href="convolutional-layers/multi-features.html">convolutional-layers/multi-features.png</a></li>
        
        <li><a href="convolutional-layers/filter.html">convolutional-layers/filter.png</a></li>
        
        <li><a href="convolutional-layers/max-pool.html">convolutional-layers/max-pool.png</a></li>
        
        <li><a href="convolutional-layers/max-pool-2.html">convolutional-layers/max-pool-2.png</a></li>
        
        <li><a href="convolutional-layers/dense-neuron.html">convolutional-layers/dense-neuron.jpg</a></li>
        
        <li><a href="convolutional-layers/MNIST-Matrix.html">convolutional-layers/MNIST-Matrix.png</a></li>
        
        <li><a href="convolutional-layers/index.html">convolutional-layers/index.clj</a></li>
        
        <li><a href="tensorflow-regression/regression_models.html">tensorflow-regression/regression_models.ipynb</a></li>
        
        <li><a href="tensorflow-regression/index.html">tensorflow-regression/index.clj</a></li>
        
        <li><a href="tensorflow-regression/regression_models-2.html">tensorflow-regression/regression_models.html</a></li>
        
    </ul>
</div>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    elements: [],
    skipStartupTypeset: true,
    /*
    "HTML-CSS": {
        scale: 83,
    },
    */
    //showMathMenu: false,
    //zoom: "None",
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      skipTags: ["script", 
      "noscript", 
      "style", 
      "textarea", 
      "nomath", 
      //"pre",
      ],
      // nomathjax used by (code ...) component
      ignoreClass: "nomath|nomathjax|tex2jax_ignore", 
    },
    /*
    TeX: {
        equationNumbers: {autoNumber: "AMS"},
    },
    */
  });
</script>
<script>
(function() {
var throttle = function(type, name, obj) {
    obj = obj || window;
    var running = false;
    var func = function() {
        if (running) { return; }
        running = true;
         requestAnimationFrame(function() {
            obj.dispatchEvent(new CustomEvent(name));
            running = false;
        });
    };
    obj.addEventListener(type, func);
};
throttle("resize", "__resize__");
throttle("scroll", "__scroll__");
})();
</script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js"></script>
    <!--
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    -->
    <!-- Global Site Tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-52618243-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments)};
        gtag('js', new Date()); 
        gtag('config', 'UA-52618243-2');
    </script>


<!--
Prism from CDN does not come with line number plugin
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.6.0/prism.min.js"></script>
-->
<script src="../static/prism/prism.js"></script>
<script src="../static/prism/prism-extension.js"></script>
<script src="../static/client/main.js"></script>

<script>
    var sitemap = {"": {"children": ["calculus", "convolutional-layers", "cross-validation", "embedding", "func-vector-space", "intro", "kernel-methods", "neural-networks", "regulation", "representations", "sequence-learning", "tensorflow", "tensorflow-regression", "tensors", "vector-spaces"], "content": "{:title \"Machine Learning\"\n :rank \"4050\"\n :label \"4050\"\n :categories [:course]\n :summary #md {{{\n                 Machine learning is a branch of Computer Science that enables machines to identify patterns, make predictions and organize data by synthesizing models of the world through learning.  In this course, we will cover the theory and application of machine learning.  We will provide a survey of the fundamental building blocks of machine learning covering areas such as general probabilistic models and parameter estimation, regression models, statistical data analysis, neural networks and neural computation.\n              }}}\n}\n", "istop": false, "path": ""}, "calculus": {"children": [], "content": "{:title \"Calculus\"\n :rank 6\n :summary #md {{{\n                 We will introduce the basics of Calculus\n                 and its application to optimization.  We also show\n                 how optimization is central to _learning_. \n                 }}}\n }\n\n\n(page #md {{{ \n            # Functions\n\n            $\\mathbf{R}$ denotes the real numbers.\n\n            $f: \\mathbf{R}\\to\\mathbf{R}$ represents some function mapping\n            numbers to numbers\n\n            Some examples are:\n\n            $f:\\mathbf{R}\\to\\mathbf{R}: x\\mapsto 3 x^3 + 2x +1$\n\n            ---\n\n            For convenience, we often write:\n\n            $f(x) = 3x^3 + 2x + 1$\n\n            to describe the function over a single variable.\n          }}})\n\n\n\n\n\n\n(page #md {{{ \n            # Derivatives\n\n            $\\frac{df}{dx}$ is the function obtained by differentiating $f(x)$.  It\u0027s also written as $f\u0027(x)$ \n            (when $f$ is a single-value function).\n\n            At each value $x$, $f\u0027(x)$ is the *rate of gain* of in $f(\\cdot)$ if we shift from $x$ to $x+\\Delta x$.\n            This reminds us the definition:\n\n            $$ f\u0027(x) = \\frac{f(x+\\Delta x) - f(x)}{\\Delta x} $$\n          }}}\n      \n      (box\n        (row\n          #md {{{\n                If all we know is $\\frac{df}{dx}(5.02) = -2.4$,\n\n                1. What is the educated guess about\n                   $f(5.01)$ versus $f(5.03)$?  What about $f(0)$ versus $f(10)$?\n\n                2. Is the educated guess always correct?\n              }}}\n          \n          (toggle\n            #md {{{\n                  (1) It\u0027s sensible to guess that $f(5.01) \u003e f(5.03)$.\n                  because it\u0027s a small $\\Delta x$ perturbation in $x$.\n\n                  But it\u0027s harder to say that $f(0) \u003e f(10)$.\n                }}}\n            \n            #md {{{\n                  (2) The educated guess can be wrong.  Can you draw\n                  a function $f$ that is smooth, but makes the guess wrong.\n                }}})))\n    ); page\n\n\n\n\n\n(page #md {{{ \n            # Optimization\n\n            _Optimization_ refers to the family of problems of finding one or many\n            values for $x$ where $f(x)$ is the maximal (or minimal).\n            }}}\n            (row 4 8\n              (image \"local-global-maxima.jpg\")\n              #md {{{\n              *Definition*\n\n              \u003e A *global maximima* for a function $f:\\mathbf{R}\\to\\mathbf{R}$\n              \u003e is a value $x^*\\in\\mathbf{R}$ such that:\n              \u003e\n              \u003e `$$\\forall x\\in\\mathbf{R},\\ f(x)\\leq f(x^*)$$`\n\n              \u003e A *local maximia* is a value $x^*$ such that\n              \u003e there exists some bound $L \u003e 0$ such that\n              \u003e\n              \u003e `$$\\forall x\\in\\mathbf{R},\\ x^*-L\\leq x\\leq x^*+L \\Rightarrow f(x)\\leq f(x^*)$$`\n          }}}))\n\n\n\n\n\n(page #md {{{ \n            # Finding local optima: a calculational approach\n\n            *Theorem*\n\n            \u003e If $x$ is a local optima and $f$ is a continuous and \"_smooth_\", then\n            \u003e $f\u0027(x) = 0$.}}} \n      #md {{{ \n            This provides a theoretically sound way to find the minimum of a function:\n\n            1. Compute $f\u0027(x)$\n            2. Solve for the roots of $f\u0027(x) = 0$\n            3. Examine to locate the one that minimizes $f$.\n            }}} \n      #md {{{ \n            But this is impractical for many reasons:\n\n            1. $f\u0027(x) = 0$ may not be easily solvable.\n            2. There may be infinitely many roots to $f\u0027(x) = 0$.\n            }}})\n\n\n\n(page (:h1 \"Finding local optima: an algorithmic solution\") \n      #md {{{ \n            Recall:\n\n            $f\u0027(x)$ points to the direction in the neighbourhood\n            of $x$ that will increase $f(x)$.  Furthermore, $-f\u0027(x)$\n            points to the direction of minimizing $f(x)$.\n\n            This provides an algorithmic way to compute a local _minimum_ of $f$.\n\n            1. Let $x_1 = c$, where $c$ is some (randomly) selected starting point.\n            2. We assume that $f\u0027$ is known. We decide how to update $x_1\\to x_2$\n               by following the opposite direction $f\u0027(x_1)$.\n\n               `$$ x_2 = x_1 - \\alpha\\cdot f\u0027(x_1) $$`\n\n               where $\\alpha$ is how far we are willing to go with $f\u0027(x_1)$.\n            }}} \n      (row 4 8 \n           (:div) \n           (box \n             #md {{{`$\\alpha$` is known as the learning rate.}}}))\n      #md {{{ \n            We can continue with the updates by moving closer to a **local** minima:\n\n            `$$x_i = x_{i-1} - \\alpha\\cdot f\u0027(x_{i-1})$$`\n\n            For sufficiently small learning rate, we end up with:\n\n            `$$ f(x_{i+1}) \\leq f(x_i) $$`\n          }}} \n      (row 4 8 \n           (:div) \n           (box \n             #md {{{\n                    The termination condition is usually that the updates \n                    no longer improves $f$.\n\n                    For example, we can set the termination condition as:\n\n                    `$$ |f(x_{i}) - f(x_{i+1})| \\leq \\epsilon $$`\n                  }}})))\n\n(page (:h1 \"Some notations with optimization\")\n  #md {{{ \n         $\\newcommand{\\argmin}{\\mathrm{argmin}}$\n\n         Consider $f:\\mathbf{R}\\to\\mathbf{R}$, and $A\\subseteq\\mathbf{R}$.\n \n         The smallest possible value of $f$ over the values in $A$ is denoted by:\n \n         `$$\\min\\{f(x):x\\in A\\}$$`\n \n         The value(s) of $x\\in A$ such that $f(x) = \\min\\{f(x):x\\in A\\}$ is denoted as\n \n         `$$\\argmin\\{f(x):x\\in A\\}$$`\n     }}}\n  \n  (box \n    #md {{{ \n           While `$\\min\\{f(x):x\\in A\\}$` is unique, `$\\argmin\\{f(x):x\\in A\\}$` may not be.\n    }}}))\n\n(page (:h1 \"Multivariate Calculus\")\n      #md {{{\n             A _scalar_ function is one that returns a number.  Namely, \n             $$ f : \\circ \\to \\mathbf{R} $$\n\n             We have been looking at scalar functions with a single input.\n          }}}\n      \n      #md {{{\n             We can have scalar functions with multiple inputs.\n             For example, a function with three inputs can look like this:\n\n             $$ f: \\mathbf{R}\\times\\mathbf{R}\\times\\mathbf{R} \\to \\mathbf{R}\n                : (x,y,z) \\mapsto x^2 + (y\\cdot z)/x $$\n\n             We can write it with brevity as: $f(x,y,z) = x^2 + yz/x$\n          }}}\n\n      #md {{{\n             When a function has a scalar output, it\u0027s also known as\n             a _potential field_, or simply a _potential_.\n             }}}\n      )\n\n(page (:h1 \"Vector field\")\n      #md {{{\n             Sometimes, a function can return multiple values.  For instance,\n             we can have a function:\n\n             `$$F:\\mathbf{R}\\times\\mathbf{R}\\to\\mathbf{R}\\times\\mathbf{R}$$`\n          }}}\n      (row 6 6\n           (image \"vector-field-1.gif\")\n           #md {{{\n                  Here is a vector field given by:\n\n                  $F_1(x,y) = (1, 1)$\n\n                  ---\n\n                  At each input $(x, y)$, the vector field has a direction and magnitude.\n                  The vector field $F_1$ has a constant direction $(1,1)$ and magnitude $\\sqrt{2}$.\n           }}})\n\n      (row 6 6\n           (image \"vector-field-2.jpg\")\n           #md {{{\n                  This vector field is given by:\n\n                  $F_2(x,y) = (x-y^2+2, xy+y)$\n           }}})\n\n      (box #md {{{\n                  - Vector fields are great at describing \n                    some _force_ distribution over some space, hence\n                    the term _force field_.\n                  - So, you can imagine how a particle may follow\n                    some trajectory in a force field.\n               }}}))\n\n\n\n\n\n(page (:h1 \"Gradient\")\n      #md {{{\n             Consider a scalar function $f(x,y)$.\n\n             The gradient of $f$ is a vector field in 2D, written $\\nabla f$.\n\n             $$ \\nabla f : \\mathbf{R}\\times \\mathbf{R} \\to \\mathbf{R}\\times \\mathbf{R} $$\n\n             given by:\n\n             $$\\nabla f(x,y) = \\left(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\right)$$\n      }}}\n\n      #md {{{\n             Intuitively, at each location $(x,y)$, $\\nabla f(x,y)$ \n             points toward the direction of increasing $f(x,y)$.\n      }}})\n\n\n\n\n(page (:h1 \"Optimization of multivariate scalar functions\")\n      #md {{{\n             We follow a similar approach as the single value case to minimize $f(\\mathbf x)$.\n\n             - Start at an arbitrary location $\\mathbf{x}_0 = (x,y)$.\n             - Move according to the vector field of $\\nabla f$ according\n               to the update rule:\n\n               `$$\\mathbf x_{i+1} = \\mathbf x_i - \\alpha \\nabla f(\\mathbf x_i)$$`\n             - Terminate when the gain is no longer significant.\n\n                `$$f(\\mathbf x_{i})-f(\\mathbf x_{i+1}) \\leq \\epsilon$$`\n      }}}\n      \n      #md {{{\n             This provides a numerical approximation to the optimization\n             problem of solving for:\n\n             `$$\\argmin\\{f(x,y): (x,y)\\in\\mathbf{R}^2\\}$$`\n      }}})\n\n(page (:h1 \"Learning as optimization\")\n\n      (rows 4 8\n\n            (:h2 \"Parameterized function\")\n            (:div #md {{{\n                         $Y = f(X|\\theta)$\n\n                         - $f$ is the function.\n                         - $X$ is the input.\n                         - $Y$ is the output.\n                         - $\\theta$ is the parameter.\n                         }}}\n                  )\n\n            (:h2 \"Learning\")\n            #md {{{\n                   We are given $(X, Y_\\mathrm{true})$, and want to\n                   compute the optimal parameter $\\theta$.\n                   }}}\n\n            (:h2 \"Loss function\")\n            #md {{{\n                   $$L: \\mathrm{range}(f)\\to\\mathrm{range}(f)\\to \\mathbf{R}$$\n                   It measures the dissimilarity between outputs.\n                   }}}\n\n            (:h2 \"Learning as an optimization\")\n            (:div #md {{{\n                         We first need to formulate the loss with respect to the\n                         model parameter $\\theta$.\n\n                         $$\\newcommand{\\true}{\\mathrm{true}}\n                         \\newcommand{\\pred}{\\mathrm{pred}}\n                         L(\\theta) = L(Y_\\true, f(X|\\theta))$$\n\n                         This defines a potential field over the parameter space.\n                         }}}\n                  #md {{{\n                         This allows us to compute the gradient:\n\n                         $$\\nabla L(\\theta)$$\n\n                         This forms a vector field over the parameter space, which allows us to apply\n                         the gradient descent algorithm with some\n                         learning rate to find a local minimum.\n                         }}}\n                  )\n            )\n      )\n", "istop": false, "path": "calculus"}, "convolutional-layers": {"children": [], "content": "{:title \"Spatial Learning With Convolution\"\n :rank 11\n :summary #md {{{\n                 Convolution Neural Networks (CNN) exploit the spatio relationship\n                 in the feature vector.  Not only can CNN better learn the underlying\n                 model, it can do so with far fewer model parameters.  The lower\n                 model complexity of CNN makes them faster to train, and with\n                 better regulation.\n                 }}}\n }\n\n(:h1 \"Image Recognition\")\n\n(page (:h1 \"Learn about images\")\n\n      #md {{{\n             Suppose that we want to train a neural network\n             to classify images.\n\n             `$$ \\mathrm{classify} : \\mathrm{Images}\\to\\mathrm{Labels} $$`\n             }}}\n\n      #md {{{\n             The classical example is the MNIST digit recognition problem.\n             }}}\n\n      #md {{{\n             While MLP can achieve very high accuracy, it\u0027s very much a \n             brute force method.\n             }}}\n      #md {{{\n             In this section we want to explore a more powerful network architecture\n             to exploit the spatio features of images.\n             }}}\n\n      (:h2 \"Why not MLP?\")\n\n      (rows :sep 6 6\n            #md {{{\n                   Consider an input image as 24x24 matrix.\n                   }}}\n            (image \"MNIST-Matrix.png\")\n\n            #md {{{\n                   MLP flattens the 2D tensor into a vector\n                   of size 784.\n\n                   The dense layers of MLP treat all pixels\n                   the same.  Since the layers are _dense_,\n                   all pixels are considered _together_.\n\n                   }}}\n            (image \"dense-neuron.jpg\")\n            ); rows\n\n      (:h2 \"Spatio-relations\")\n\n      #md {{{\n             Images (like many other input data) have spatio-relations\n             between its pixels.\n\n             Denote the image tensor as `$I\\in\\mathbb{R}^{28\\times 28}$`.\n             }}}\n\n      #md {{{\n             Each pixel $I(i,j)$ has natural neighbours, such as\n             $I(i+1, j+1)$.  We can define a neighbourhood around $(i,j)$\n             of size 4 pixels as:\n\n             `$$\\mathbf{N}_4(i, j|I) = \\{I(i+d_i, j+d_j): d_i\\in[0,3], d_j\\in[0, 3]$$`\n             }}}\n\n      (rows 6 6\n            #md {{{\n                   Can we hope to learn useful local features\n                   by only training with respect to a neighbourhood?\n                }}}\n            (image \"neighbour-neuron.jpg\")\n            ); rows\n      ); page\n\n(page (:h1 \"Getting neighbourhoods\")\n\n      (rows :sep 4 8\n            #md {{{\n                   Let\u0027s start an image\n                   tensor of size $n\\times n$.\n                   }}}\n            #md {{{\n                   $$I\\in\\mathbb{R}^{n\\times n}$$\n                   }}}\n\n\n            #md {{{\n                   We can define a neighbourhood by a\n                   square over the indexes of $I$.\n                   }}}\n            #md {{{\n                   $$N_k(i,j|I)$$\n\n                   We say that if $i+k\\geq n$ or $j+k\\geq n$,\n                   the neighbourhood is _not defined_.\n                   }}}\n\n\n\n            #md {{{\n                   How many neighbourhoods are there in $I$?\n                   }}}\n            #md {{{\n                   Counting:\n\n                   \u003e - If $n = k$, then we can only have one neighbourhood.\n                   \u003e - If $n = k+1$, then we can shift horizontally or vertically,\n                   \u003e so we get $2\\times 2=4$ neighbourhoods.\n                   \u003e\n                   \u003e In generally, we have $(n-k+1)\\times (n-k+1)$ neighbourhoods.\n                   }}}\n\n            #md {{{\n                   What are the neighbourhoods?\n                   }}}\n            #md {{{\n                   Enumerating:\n\n                   \u003e `$\\mathcal{N}(I, k) = \\{N_k(i, j|I): i\\in\\mathrm{range}(0, n-k+1), j\\in\\mathrm{range}(0, n-k+1)\\}$`\n                   }}}\n\n\n            #md {{{\n                   Strides:\n\n                   \u003e We can generalize the way we produce the neighbourhoods by making the\n                   adjacent neighbourhoods _less overlapping_.\n                   }}}\n            (:div #md {{{\n                         Enumeration with stride $s$:\n\n                         \u003e `$\\begin{eqnarray}\n                           \\mathcal{N}(I, k, s) \u0026=\u0026 \\{N_k(i, j|I): \\\\\n                           \u0026\u0026 \\quad\\quad i\\in \\mathrm{range}(0, s, n-k+1), \\\\\n                           \u0026\u0026 \\quad\\quad j\\in \\mathrm{range}(0, s, n-k-1)\\}\n                           \\end{eqnarray}$`\n                         }}}\n                  #md {{{\n                         Strides _decrease_ the number of neighbourhoods.\n                         }}})\n\n            (:div\n                  #md {{{\n                         The power of neural networks (as demonstrated by MLP)\n                         is the multiple layers. But even with stride $s=1$, the input\n                         tensor size is reduced by $k-1$ along each dimension.\n                         }}}\n                  #md {{{\n                         Padding:\n\n                         \u003e If we first increase the size of the input tensor,\n                         and then perform neighbourhood generation, then\n                         we can have $n^n$ number of neighbourhoods instead of\n                         $(n-k+1)^2$.\n                         }}})\n            (:div\n              #md {{{\n                     *Definition*: Padding\n\n                     Let $I$ be a 2D tensor of size $n\\times n$.  A padded version of $I$,\n                     $J = \\mathrm{pad}(I, m)$, is obtained by enlarging $I$ by $m$ along each\n                     dimension filling the new entries using the constant $0$.\n\n                     \u003csmall\u003e\n                     `$$J(i,j) = \\left\\{\\begin{array}{ll}\n                        I(i-m,j-m) \u0026 \\mathrm{if}\\ i\\in[m, n+m-1]\\ \\mathrm{and}\\ j\\in[m,n+m-1] \\\\\n                        c \u0026 \\mathrm{otherwise}\n                        \\end{array}\\right.\n                      $$`\n                      \u003c/small\u003e\n                     }}}\n              #md {{{\n                     Enumeration with padding.\n\n                     \u003e `$\\mathcal{N}(I, k, s, p=\\mathrm{True}) = \\{ N_k(i,j|\\mathrm{pad}(I,k)): i, j\\in [0, n-1] \\}$`\n                     }}}\n              ); div\n            ); rows\n      #md {{{\n             In summary, we have a number of different ways of enumerating the neighbourhoods:\n\n             - Convolution:\n\n                \u003e `neighbourhoods(I, window_size=k)`\n\n\n             - Convolution with stride:\n\n                \u003e `neighbourhoods(I, window_size=k, stride=s)`\n\n             - Convolution with stride and padding:\n\n                \u003e `neighbourhoods(I, window_size=k, stride=s, padding=True)`\n             }}}\n      ); page\n\n(page (:h1 \"Convolutional Network With Filter\")\n\n      #md {{{\n             *Motivation*:\n\n             We hope that we can discover a pattern _in the neighbours_\n             that can effectively determine the correct class of the image.\n             }}}\n\n      (row 6 6\n          #md {{{\n                 - Each neighbourhood is a vector of size $k^2\\times 1$, call it $\\mathrm{nb}_{ij}\\in\\mathbb{R}^{k^2}$.\n                 - Each neighbourhood is connected to a neuron of the next layer.\n                 - Each connection is a dense layer with weights $W\\simeq k\\times k$, and bias $b$, and\n                 some activation (typically rectilinear activation).\n                 }}}\n          (image \"filter.png\"\n                 #md {{{\n                        From [Nielsen, 2017](http://neuralnetworksanddeeplearning.com/chap6.html)\n                        }}}))\n\n      (box #md {{{ \n                  *Filter*\n\n                  \u003e - Every weight is *the* same.  This is so that we look for the _same_ feature\n                  \u003e across all the neighbourhoods.\n\n                  \u003e - The weight $W\\simeq k\\times k$ is called the filter.\n                  \u003e \n                  \u003e - The _same_ filter is applied across the entire image by the neighbourhood generation\n                 (convolution). \n                  }}})\n\n      (:h2 \"Multiple filters\")\n\n      (row 5 7 \n           #md {{{\n               Each filter is a _feature_ that we want to detect\n               to determine the image classification.\n\n               We can look for multiple features by using multiple filters.\n               }}}\n           (image \"multi-features.png\"\n                  #md {{{\n                         From [Nielsen 2017](http://neuralnetworksanddeeplearning.com/chap6.html)\n                         }}}))\n      ); page\n\n(page (:h1 \"Max-pooling\")\n\n      #md {{{\n             The intuition behind `conv2d` is to\n             detect the presence the feature by\n             sliding the filter.\n\n             The output of `conv2d` is a tensor\n             of dimension:\n\n             `(None, height, weight, n_features)`\n\n             Each one of the `n_features` tensor\n             is called a _feature map_.\n             }}}\n\n      #md {{{\n             _Max-pooling_ is an additional layer\n             in the neural network that condenses\n             the feature layer further.\n             }}}\n\n      (rows :sep 5 7\n            #md {{{\n                   Max-pooling generates \n                   neighbourhoods of size $p\\times p$\n                   with possible strides, typically\n                   $2\\times 2$ with stride=2 in the feature map.\n\n                   Only the maximum output in each neighbourhood is\n                   kept.  So, the max-pooling layer makes use\n                   of the `max-out` activation function on each\n                   neighbourhood.\n                   }}}\n            (image \"max-pool.png\"\n                   #md {{{\n                          Source: [Nielsen 2017](http://neuralnetworksanddeeplearning.com/chap6.html)\n                          }}})\n\n            #md {{{\n                   The effect of max pooling is that the\n                   number of neurons is greatly reduced while\n                   still preserving the most salient features.\n                   }}}\n            (image \"max-pool-2.png\"\n                   #md {{{\n                          Source: [Nielsen 2017](http://neuralnetworksanddeeplearning.com/chap6.html)\n                          }}})\n            ); rows\n      ); page\n\n(page (:h1 \"Completing the task of classification\")\n\n      #md {{{\n             We have the following architecture so far:\n\n             ```\n             Batch         Conv          Max\n             MNIST    --\u003e  Net   -----\u003e  Pooling\n             Image         (5x5)         (2x2)\n\n             (-1,28,28,1) (-1,24,24,32) (-1,14,14,32)\n             ```\n             }}}\n\n      #md {{{\n             The objective is to reduce the dimensionality\n             from (-1, 14, 14, 32) to (-1, 10)\n             so that softmax activation can be used\n             to determine the image label (0-9).\n\n             This can be done with a dense layer.\n             }}}\n\n      #md {{{\n             ```\n             Batch         Conv          Max          Dense\n             MNIST    --\u003e  Net   -----\u003e  Pooling  --\u003e Softmax\n             Image         (5x5)         (2x2)\n\n             (-1,28,28,1) (-1,24,24,32) (-1,14,14,32) (-1, 10)\n             ```\n             }}}\n \n      ); page\n\n(page (:div\n        {:style {:font-size \"140%\"\n                 :line-height \"200%\"}}\n        (:h1 \"Summary\")\n        #md {{{\n               - Use neighbourhoods to capture local features.\n\n               - Generate neighborhoods using convolution, strides and padding.\n\n               - Generate more than one feature to make up for the loss\n               on model complexity.\n\n               - Use max-pooling to reduce the neurons.\n\n               - Use dense layer to perform softmax classification.\n               }}}))\n", "istop": false, "path": "convolutional-layers"}, "cross-validation": {"children": [], "content": "{:title \"Cross Validation\"\n :rank 10\n :summary #md {{{\n                 We will look into the risk of _overfitting_\n                 during training, and how we can avoid overfitting\n                 using cross-validation.\n              }}}}\n\n(:h1 \"Machine Learning Revisited\")\n(page #md {{{\n             Let\u0027s review the various components of the learning\n             framework.\n          }}})\n\n(page (:h1 \"The Model Architecture\")\n\n      #md {{{\n            The architecture of a learning model is the general mathematical\n            structure that performs data processing.\n          }}}\n\n      #md {{{\n             So far, we have seen several times of models:\n\n             - Single layer\n\n             - Multi-layer perceptron with different\n             layers and activation functions.\n          }}}\n\n      #md {{{\n            We denote the _architecture_ of a learning\n            model as $M$.\n          }}}\n      )\n\n(page (:h1 \"The Model Parameter\")\n      #md {{{\n             Recall that MLP requires the weights of all the layers,\n             and Rosenlatt\u0027s perceptron requires the linear separating\n             vector to function.\n\n             These numerical weights are called the _model parameter_.\n             We write them as a vector: \n             $\\theta$.\n          }}}\n\n      #md {{{\n             The significance is that a model can only function\n             with some specific parameters.\n\n             $$M[\\theta] : \\mathrm{Input} \\to \\mathrm{Output}$$\n          }}}\n\n      (:h2 \"Model Complexity\")\n      \n      #md {{{\n             Different architectures require different number of model\n             parameters.  The number of parameters required to fully\n             specify a working model $M$ is called the\n             _model complexity_.\n          }}}\n      )\n\n(page (:h1 \"Learning\")\n      #md {{{\n             Learning is possible if we have a set of _training_\n             data \n             `$$T=\\{\\left\u003cx_i, y_i\\right\u003e: i \u003e 0\\}$$`\n             where\n             - `$x_i$` is the input, and\n             - `$y_i$` is the expected output.\n\n             Alternatively, we may also write the training data\n             as a tensor:\n\n             $$ T = [X, Y] $$\n          }}}\n\n      (:h2 \"Training error\")\n\n      #md {{{\n             $\\newcommand{\\err}{\\mathrm{err}}$\n             We can assess the performance of a model and its\n             parameters by the error:\n\n             `$$ \\err(T|\\theta, M) = \\mathrm{cost}(Y, M[\\theta](X))$$`\n          }}}\n\n      #md {{{\n             If the model architecture is fixed, we can write the error\n             as:\n\n             $$\\err(T|\\theta)$$\n          }}}\n\n      (:h2 \"Learning\")\n      \n      #md {{{\n             _Learning_ is the process of tuning the parameters to minimize the\n             error with respect to some training data:\n\n             `$$\\theta^* = \\underset{\\theta}{\\mathrm{argmin}}\\big[ \\err(T|\\theta)\\big] $$`\n          }}}\n      )\n\n(:h1 \"The Danger of Learning\")\n\n(page (:h1 \"Training Data\")\n      #md {{{\n             Training data $T=[X, Y]$ almost inevitably has two imperfections that\n             we must detail with:\n          }}}\n\n      (row 6 6\n           #md {{{\n                  # Noise\n\n                  The training data is typically collected\n                  from past experience.  There will always\n                  be noise in the training data.\n\n                  Thus,\n\n                  `$$ T = [X, Y+\\mathbf{e}] $$`\n\n                  \u003e where $\\mathbf{e}$ is the noise.\n               }}}\n           (:div\n             (image \"no-noise.png\" #md \"Observation without noise\")\n             (image \"with-noise.png\" #md \"Observation with some noise\"))\n           )\n\n      (row 6 6\n           #md {{{\n                  # Incompleteness\n\n                  Because training data is often collected\n                  (as opposed to) generated.\n                  One cannot expect that it contains all\n                  possible input/output pairs that the neural\n                  network will encounter.\n               }}}\n\n           (image \"with-missing.png\" #md \"Missing observations\"))\n      )\n\n(page (:h1 \"Ideal learning\")\n      #md {{{\n             The learning algorithm should exhibit the following\n             characteristics:\n\n             1. Noise rejection.\n\n             2. Generalization to unseen data.\n          }}}\n\n      (:h2 \"Noise rejection\")\n      #md {{{\n             The eventual model should learn the mapping\n             from $X$ to $Y$, but not $X\\mapsto Y+\\mathbf{e}$.\n\n             Thus, even if the training data is given as:\n\n             `$$x_i = y_i + e_i$$`\n\n             we want the model to predict:\n\n             `$$M[\\theta](x_i) \\simeq y_i $$`\n          }}}\n\n      (:h2 \"Generalization\")\n\n      #md {{{\n             Generalization refers to the ability that\n             the learning algorithm is learning a _general_ rule\n             of mapping the inputs to the output, as opposed to\n             a very specific case-by-case mapping.\n          }}}\n\n      #md {{{\n             When presented with an input $x\\not\\in X$,\n             the model should perform \"well\" with a reasonable\n             `$M[\\theta](x)$`.\n          }}}\n      )\n\n(page (:h1 \"Evidence of Overfitting\")\n      (row 6 6\n           #md {{{\n                  Consider the training data as given in the figure\n                  on the right.\n\n                  This data set is generated by:\n\n                  - There are 100 pairs of points\n                    `$$\\left[\\begin{array}{cc}\n                      x_1 \u0026 y_1+e_1 \\\\\n                      x_2 \u0026 y_2+e_2 \\\\\n                      \\vdots \u0026 \\vdots \\\\\n                      x_{100} \u0026 y_{100}+e_{100} \\end{array}\\right]$$`\n                  - `$x_i\\in[-1, 1]$`\n                  - `$y_i = (3x_i^2 + 2x_i + 1)/6$`\n                  - `$e_i$` are 0.5-spikes at random positions.\n               }}}\n           (image \"with-noise.png\"))\n\n      (row 6 6\n           (image \"overfitted.png\"\n                  #md {{{\n                         \u003ccenter\u003e(a)\u003c/center\u003e\n                         \n                         This is trained by a MLP with the architecture\n                         of `[100 $\\times$ 100 $\\times$ 10]` with\n                         10,000 epochs.\n\n                         Note that the learning is being affected\n                         by the presence of the spiky noise in the data\n                      }}})\n           (image \"overfitted-cost.png\"\n                  #md {{{\n                         \u003ccenter\u003e(b)\u003c/center\u003e\n\n                         We can see the training error has not significantly\n                         improved since epoch $\\sim$ 1000.\n\n                         Over learning *and* model complexity are responsible\n                         for the overfitting.\n                      }}}))\n\n      (box {:style {:border \"thin solid crimson\"}}\n           #md {{{\n                 One should be _very_ concerned with figure (a) and (b).\n\n                 - Figure (a) shows that the inevitable presence of noise\n                 will ruine the usefulness of the resulting neural network.\n\n                 - **However**, figure (b) does not show that there is\n                 anything wrong with the training process.  The training\n                 error suggests that we can still keep learning.\n\n                 \u003e ** How do we detect _overfitting_? **\n\n                 \u003e ** How do we prevent _overfitting_? **\n               }}})\n      ); page\n\n(:h1 \"Testing and Cross Validation\")\n\n(page (:h1 \"Testing Error\")\n      #md {{{\n            Let\u0027s evaluate the model is to assess the accuracy\n            _after_ the learning process has completed.\n\n            We can do so with a set of _test data_.\n          }}}\n\n      (box #md {{{\n                  _Test Data_:\n\n                  \u003e Test data is a set `$S = \\{(x_i, z_i): i = 1, 2, \\dots\\}$`\n                  such that\n                  - `$x_i$` is the input,\n                  - `$z_i$` is the expected output.\n                  \u003e\n                  \u003e Test data can _never_ be used during learning.  This is\n                  the only and crucial difference from _training data_.\n\n                  _Test Error_:\n\n                  \u003e The test error is defined as:\n                  \u003e\n                  \u003e `$$ \\err(S|\\theta, M)$$`\n               }}})\n      ); page\n\n(page (:h1 \"Overfitting revisited\")\n      (row 6 6\n           (image \"test-err-final.png\"\n                  #md {{{\n                         \u003cbr\u003e\n                         This is the test error after\n                         10,000 epochs of the three-layer MLP.\n                      }}})\n           (image \"test-err.png\"\n                  #md {{{\n                         \u003cbr\u003e\n                         This is how the test error\n                         changed during the learning phase.\n                      }}}))\n      #md {{{\n             It because immediately obvious that the neural network\n             is actually _over-learning_, leading to overfitting.\n\n             We can see that any learning pass 1,000 epochs are\n             worsening the test error.\n          }}}\n\n      (note #md {{{\n                   **We cannot never change the model _after_\n                   testing.**\n                }}})\n      ); page\n\n(page (:h1 \"Cross Validation\")\n      #md {{{\n             Cross validation is a technique that uses the training data intelligently\n             to:\n\n             1. Train the parameters $\\theta$ of the model,\n             2. and evaluate the \"test error\" to avoid overfitting.\n          }}}\n\n      #md {{{\n             We must first formalize the training algorithm:\n\n             ```\n             def train($T$, $\\theta_0$):\n             \u2502  $\\theta \\leftarrow \\theta_0$\n             \u2502  for $i: 1\\to n$:\n             \u2502  \u2502   $\\theta \\leftarrow \\mathrm{argmin}_\\theta\\big[\\err(T|\\theta)\\big]$\n             \u2502  return $\\theta$\n             ```\n          }}}\n\n      #md {{{\n            Training data is precious and we need all of it to perform training.\n            Cross validation samples a portion of the training data $T$ as test data.\n            This sample is _different_ for each epoch of the training.\n          }}}\n\n      (box #md {{{\n                  _In-training Test Data_\n\n                  \u003e Let $T$ be the training data.  We define:\n                  \u003e\n                  \u003e `$$\\begin{eqnarray}\n                  \u003e  D_\\mathrm{cv} \u0026=\u0026 \\mathrm{sample}_n(T) \\\\\n                  \u003e  D_\\mathrm{train} \u0026=\u0026 T - D_\\mathrm{cv}\n                  \u003e  \\end{eqnarray}$$`\n\n                  \u003e where `$\\mathrm{sample_n}$` samples $n$ training\n                  pairs $(x, y)$.\n\n                  \u003e - $n$ can be a percentage of the total available\n                  training data $$n = p\\cdot|T|,\\ p\\in[0, 1]$$\n                  \u003e - $n$ can be as small as $n=1$.  This is called _leave-one-out_\n                  cross validation.  There is also _leave-$k$-out_ cross validation.\n               }}})\n\n      #md {{{\n             Let\u0027s rewrite the training algorithm that uses leave-k-out cross validation.\n          }}}\n      (row 6 6\n           #md {{{\n             ```\n             def cv(T, k):\n             \u2502  S = sample(T, k)\n             |  return T-S, S\n             ```\n             }}}\n           #md {{{\n                  The `CV` function returns the training data and test data to be used.\n               }}})\n      (row 6 6\n           #md {{{\n                  ```\n                  def cv_train(T, $\\theta_0$, k):\n                  \u2502  $\\theta \\leftarrow \\theta_0$\n                  \u2502  for $i: 1\\to n$:\n                  |  \u2502  $D_\\mathrm{train}, D_\\mathrm{cv}$ = sample(T, k)\n                  \u2502  \u2502  $\\theta \\leftarrow \\mathrm{argmin}_\\theta\\big[\\err(D_\\mathrm{train}|\\theta)\\big]$\n                  \u2502  \u2502  $e_\\mathrm{cv} = \\err(D_\\mathrm{cv}|\\theta$)\n                  \u2502  \u2502 \n                  \u2502  \u2502  if $e_\\mathrm{cv}$ increases:\n                  |  |  \u2502  break\n                  |  return $\\theta$\n                  ```\n               }}}\n           #md {{{\n                  This modified training algorithm prevents overfitting\n                  by monitoring the test error during the learning phase.\n               }}})\n\n      (row 6 6 \n           (image \"regulation-learning.png\" \n                  #md {{{\n                         \u003cbr\u003e\n                        This is the performance of the model\n                        with optimal number of epochs ($\\sim 800$). \n                      }}})\n           (image \"overfitted.png\"\n                  #md {{{\n                         \u003cbr\u003e\n                         You can see that the regulated learning is much better\n                         than this overfitted learned model with 10,000 epochs.\n                      }}}))\n      ); page\n\n(page (slide (:h1 \"Summary\")\n             #md {{{\n                    **Overfitting** occurs when a model mistakes _noise_\n                    as _signal_, and end up making predictions based on the\n                    _noises_ in the observation.\n                    }}}\n             (--*--)\n             #md {{{\n                    **Overfitting happens when**:\n\n                    - The model is _too_ large for the problem at hand.\n                    - The model has been training for too long (epochs too large).\n                    }}}\n             (--*--)\n             #md {{{\n                    **Cross Validation**\n\n                    - Detects the occurrence of overfitting.\n                    }}}\n             )\n      )\n", "istop": false, "path": "cross-validation"}, "embedding": {"children": [], "content": "{:title \"Embedding Vectors\"\n :rank 14\n :author #md {{{\n                - Ken Pu\n                - Faculty of Science, UOIT\n                - Copyright, 2017\n                }}}\n :summary #md {{{\n                 In this section, we survey several methods\n                 of mapping inputs of different data types\n                 into a high dimensional _vector space_.\n                 }}}\n }\n\n(:h1 \"Motivations For Embeddings\")\n\n\n(page (banner #md {{{\n               Embedding is a powerful technique that enables\n               neural networks be applied to input data beyond\n               just vectors from a given vector space.\n               }}})\n\n      (rows :sep 4 8\n            (:h2 \"Dealing with complexity\")\n            (:div #md {{{\n                         Which would be a _harder_ learning problem?\n\n                         - Classification of inputs from $\\mathbb{R}^2$ ?\n\n                         - Classification of inputs from $\\mathbb{R}^{1000}$ ?\n                         }}}\n                  #md {{{\n                         Computational complexity is _likely_ higher for\n                        the case of $\\mathbb{R}^{1000}$, as there are more\n                        model parameters to tune.\n                        }}}\n\n                  #md {{{\n                         ### Embedding of high dimensional vectors:\n\n                         \u003e What if we can find some mapping:\n                         \u003e \n                         \u003e $$h: \\mathbb{R}^{1000}\\mapsto\\mathbb{R}^{10}$$\n                         \u003e\n                         \u003e such that $h(\\mathrm{input})$ preserves the\n                         \u003e significant information to perform learning task?\n                         }}}\n                  )\n\n            \n            (:h2 #md \"Dealing with __large__ categorical domains\")\n            (:div #md {{{\n                         So far, the only way we have seen to encode categorical\n                         values is to encode them as _one-hot_ vectors\n                         with dimensionality of $|\\mathrm{dom}|$.\n                         }}}\n                  #md {{{\n                         What if the domain is 1000,000?\n\n                         \u003e Think of the distinct words from 43 million Wikipedia pages.\n                         }}}\n                  #md {{{\n                         Wouldn\u0027t it be great if we just input a categorical value\n                         by its __index__ (from 0 to $|\\mathrm{dom}|-1$).\n                         }}}\n                  )\n            ); rows\n      ); page\n\n(:h1 \"Autoencoder\")\n\n(page (:h1 \"Redundancy\")\n      #md {{{\n             MNIST images are 784 dimensional vectors.\n\n             - 28 $\\times$ 28\n             - what if they are higher resolution (640$\\times$ 640)?\n             }}}\n\n      #md {{{\n             We don\u0027t need the high resolution to\n             perform classification.\n             }}}\n\n      #md {{{\n             This suggests that there is a very high degree\n             of redundancy in the vector representation of\n             images, even at the low resolution of $28\\times 28$.\n             }}}\n\n      (box #md {{{\n                  1. Can a lower (say 10) dimensional representation be sufficient\n                  for image recognition for the MNIST dataset?\n\n                  2. If so, how do we reduce the 784 dimension down to \n                  a smaller number of dimensions?\n                  }}})\n      ); page\n\n(page (:h1 \"Encoding\")\n\n      (rows :sep 4 8\n            (:h2 \"Input data\")\n            #md {{{\n                   Consider a data set $X\\subseteq\\mathbb{R}^D$.\n                   For instance, the MNIST images are vectors with $D=784$.\n                   }}}\n\n\n            (:h2 \"Transformation\")\n            #md {{{\n                   We consider a linear transformation from $\\mathbb{R}^D$\n                   to a lower dimensional space $\\mathbb{R}^d$ where $d \u003c D$.\n\n                   $$ \\mathbf{u} = h(\\mathbf{x}) = W^T\\cdot\\mathbf{x} + \\mathbf{b} $$\n                   }}}\n\n            (:h2 \"Encoding the data\")\n            #md {{{\n                   $$ U = h(X) $$\n                   }}}\n            ); rows\n      ); page\n\n(page (:h1 \"Decoding\")\n\n      (rows :sep 4 8\n            (:h2 \"Reconstruction transformation\")\n            (:div #md {{{\n                         The objective is that we can reconstruct the original\n                         data $X$ with minimal error.\n                         }}}\n                  #md {{{\n                         We can assume a reconstruction linear transformation\n                         $V$ such that\n\n                         $$ \\mathbf{y} = g(\\mathbf{u}) = V^T\\cdot\\mathbf{u} + \\mathbf{c} $$\n                         }}})\n            ); rows\n      ); page\n\n(page (:h1 \"Automatic Encoding/Decoding By Learning\")\n\n      (rows :sep 4 8\n            (:h2 \"Have we lost information?\")\n            (:div #md {{{\n                         We can attempt to reconstruct $X$ through encoding-decoding\n\n                         $$\\mathbf{y} = g(h(\\mathbf{x}))$$\n                         }}}\n\n                  #md {{{\n                         The reconstruction error can be measured by any of the cost functions.\n\n                         For example, we can use the mean distance:\n\n                         `$$ E = \\frac{\\sum_{\\mathbf{x}\\in X} \\|g(h(\\mathbf{x})) - x\\|}{|X|}\n                         $$`\n                         }}}\n                  )\n\n\n            (:h2 \"The model and parameter\")\n            (:div #md {{{\n                        The encoder / decoder is the model $\\mathbf{M}$.\n\n                        - It\u0027s a two layer MLP.\n                        - The hidden layer outputs a lower dimensional vector.\n                        - The output layer\u0027s dimensionality = input dimensionality.\n                         }}}\n\n                  #md {{{\n                         The model parameters are:\n\n                         $$\\theta = [W, \\mathbf{b}, V, \\mathbf{c}] $$\n                         }}}\n                  )\n\n            (:h2 \"Learning optimal encoder/decoder\")\n            (:div (box #md {{{\n                              $$\\theta^* = \\mathrm{train}(\\mathbf{M} | X)$$\n                              }}})\n                  #md {{{\n                         - The encoder\u0027s parameter $(W,\\mathbf{b})$ will be learned.\n                         - The decoder\u0027s parameter $(V,\\mathbf{c})$ will be learned.\n                         }}}\n                  )\n            ); rows\n      ); page\n\n(:h1 \"Application of Autoencoder: Neural Linguistics\")\n\n(page (:h1 \"Computational Linguistics\")\n      (rows :sep 4 8\n            (:h2 \"Modeling written language\")\n            (:div #md {{{\n                         The most faithful model of written language is a _description_\n                         of how symbols from a vocabulary are put together to form sentences.\n\n                         This is the formal language modeling:\n\n                         $$ L \\subseteq \\Sigma^* $$\n                         }}}\n                  (note :side #md {{{\n                                     Recall regular languages and context-free grammar\n                                     from _Compilers_ model languages this way.\n                                     }}})\n                  #md {{{\n                         A language model is a system that can _predict_ the\n                         next symbol to come given some information about its context.\n\n                         1. `I have been studying so ____ that my head hurts.`\n                         2. `Hi, my ____ is Jack.`\n                         3. `Tomorrow, I am going to ____`\n                         4. `____`\n                         }}}\n                  )\n\n\n            (:h2 \"Neural linguistics\")\n            (:div #md {{{\n                         - Start with a vocabulary $\\Sigma$, and $n = |\\Sigma|$.\n                         - Each word is represented by an onehot vector $\\mathbf{R}^n$.\n\n                         }}}\n                  (note :side\n                        #md {{{\n                         $n$ may be prohibitively large (~100,000).  We can do something\n                         about this later.\n                         }}})\n                  #md {{{\n                         - The context is the previous few words and the next few words.\n\n                            \u003e Say we let the context be  \u003cbr\u003e\n                            \u003e `word1 word2 ____ word3 word4`.\n\n                         - We want to predict the missing word using a neural network.\n                      }}}\n                  )\n\n            (:h2 \"The input/output\")\n            (:div #md {{{\n                         - Each word is a vector `$\\mathbf{w}_i\\in\\mathbb{R}^n$`.\n                         - We can encode the _context_ as a vector by\n                         concatenating the four words together:\n                          $$\\mathrm{context} \\in\\mathbb{R}^{4n}$$\n                         }}}\n                  #md {{{\n                         - The output is the probability distribution\n                         over $\\Sigma$ for the missing word.\n                         $$y\\in\\mathbb{R}^n$$\n                         }}}\n                  )\n\n            (:h2 \"The neural network\")\n            #md {{{\n                   `$$\n                   \\mathbf{y} = \\mathrm{softmax}\\circ\\mathrm{tf.dense}(\\mathrm{context})\n                   $$`\n                   }}}\n\n            (:h2 \"The training\")\n            #md {{{\n                   Training data is generated from a text corpus.\n\n                   - Wikipedia pages\n                   - Google books\n                   - ...\n                   }}}\n            )\n      (box #md {{{\n                  # The problem\n\n                  The context and the words are too high dimensional\n                  for effective training of the network.\n                  }}})\n      ); page\n\n(page (:h1 \"Along comes autoencoder\")\n      #md {{{\n             We can reduce the dimensionality of the word representation\n             if we can introduce encoding and decoding layers\n             to the network.\n             }}}\n\n      #md {{{\n             `$$\\begin{eqnarray}\n             \\mathbf{y} \u0026=\u0026 \\mathrm{softmax} \n             \\circ\\mathrm{decode} \n             \\circ\\mathrm{tf.dense}\n             \\circ\\mathrm{encode}(\\mathrm{context})\n              \\end{eqnarray}\n              $$`\n              }}}\n      ); page\n\n(:h1 \"Embedding Categorical Values\")\n\n(page (:h1 \"Categorical values as indexes\")\n      (rows :sep 4 8\n            (:h2 \"Encoding to integers\")\n            #md {{{\n                   Given a finite set of $n$ values, we can represent each value as an integer as follows\n\n                   - Sort the set by some ordering,\n                   - Each value corresponds to a unique index from $[0 .. n-1]$.\n                   }}}\n\n\n            (:h2 {:style {:color :red}} \"Cannot treat integers as floats\")\n            #md {{{\n                   It is _extremely_ tempting to feed integers corresponding to categorical\n                   values to a neural network.\n\n                   \u003e You cannot do that.  Period.\n                   }}}\n\n            (:h2 \"Embedding layer\")\n            (:div #md {{{\n                         We introduce the _embedding_ layer.\n\n                         $$\\mathrm{Embedding} : \\mathbb{N} \\to\\mathrm{R}^d$$\n\n                         - The inputs are integers.\n                         - The outputs are $d$-dimensional vectors.\n                         }}}\n                  #md {{{\n                         The embedding layer generates $d$-dimensional representation\n                         of the categorical integer encodings.\n                         }}}\n                  #md {{{\n                         The model parameter is given by a 2D matrix:\n                         $$ W\\in\\mathbb{R}^{n\\times d} $$\n                         }}}\n                  #md {{{\n                         The action of this layer is __not__ matrix multiplication, but rather\n                         a row lookup:\n\n                         $$ \\mathbf{y} = W_i $$\n                         }}}\n                  )\n            )\n      ); page\n\n(page (:h1 \"Learning with Embedding Layer\")\n      #md {{{\n             The embedding layer is a building block of a more complex\n             network.\n\n             - There is no learning objective.\n             - There is no cost function.\n             }}}\n\n      #md {{{\n             However, as part of a larger network, the embedding layer\n             can be trained to perform _embedding_ of categorical values.\n             }}}\n\n      (:h2 \"Learning from categorical data\")\n      (rows :sep 4 8\n            (:h2 \"The task\")\n            (:div #md {{{\n                         Suppose that we have a data set on airline flight\n                         information.\n\n                         | Departure City | Arrival City | Airline | Status |\n                         |----------------|--------------|---------|--------|\n                         | Toronto        | Montreal     | Porter  | On-time|\n                         | $\\vdots$       | $\\vdots$     | $\\vdots$|$\\vdots$|\n                         }}}\n\n                  (--*--)\n\n                  #md {{{\n                         If we want to perform prediction on the flights status\n                         based on known information:\n\n                         \u003e What is the likelihood that a _Air Canada_\n                         \u003e flight from\n                         \u003e _Montreal_ to _Vancouver_ will be _late_?\n                         }}}\n                  )\n            (:h2 \"Encoding of categorical domains\")\n            (:div #md {{{\n                         _City_:\n\n                         ```\n                         0    Toronto\n                         1    Montreal\n                         2    Vancouver\n                         3    Halifax\n                         ```\n\n                         _Airline_:\n\n                         ```\n                         0    Porter\n                         1    Air Canada\n                         ```\n\n                         _Status_:\n\n                         ```\n                         0    On-time\n                         1    Late\n                         2    Early\n                         4    Cancelled\n                         ```\n                         }}}\n                  #md {{{\n                         The input to the network will be simple integer vectors:\n\n                         `$$\n                         \\left[\\begin{array}{c} \n                               \\mathrm{from:\\ Montreal} \\\\ \n                               \\mathrm{to:\\ Vancouver} \\\\ \n                               \\mathrm{airline:\\ Air\\ Canada} \n                               \\end{array}\\right]\n                         \\mapsto\n                         \\left[\\begin{array}{c} 1 \\\\ 2 \\\\ 1\\end{array}\\right]$$`\n                         }}}\n                  )\n\n            (:h2 \"Embedding layers\")\n            (:div #md {{{\n                         We create two embedding layers:\n\n                         - City embedding\n                           \u003e `$$ h_1 : \\mathbb{N} \\to \\mathbb{R}^m$$`\n                           \u003e We need to make a choice on the value of $m$.\n\n                         - Air line embedding\n                           \u003e `$$ h_2 : \\mathbb{N} \\to \\mathbb{R}^n$$`\n                           \u003e Again, the value of $n$ needs to be selected.\n                         }}}\n\n                  #md {{{\n                         So the input becomes:\n\n                         `$$\\left[\\begin{array}{c}\n                                  1 \\\\ 2 \\\\ 1\n                                  \\end{array}\\right]\n                            \\mapsto\n                            \\left[\\begin{array}{c}\n                                  h_1(1) \\\\ h_1(2) \\\\ h_2(1)\n                                  \\end{array}\\right] \\in\\mathbb{R}^{2m+n}\n                            $$`\n                         }}}\n                  )\n\n            (:h2 \"Classification\")\n            (:div #md {{{\n                         The most significant function of embedding layers is that\n                         they convert discret categorical values to multidimensional\n                         vectors.\n                         }}}\n                  #md {{{\n                         We can design the rest of the neural network\n                         freely.\n                         }}}\n                  #md {{{\n                         The simpliest would be a dense layer to convert\n                         the $\\mathbb{R}^{2m+n}$ vectors to a distribution\n                         over the **four** possible status $\\mathbb{R}^4$.\n                         }}}\n                  #md {{{\n                         `$$\n                         \\mathbf{y} = \\mathrm{softmax}\\left(W_{(2m+n)\\times 4}^T \n                            \\cdot\n                            \\left[\\begin{array}{c}\n                                  h_1(1) \\\\ h_1(2) \\\\ h_2(1)\n                                  \\end{array}\\right]\n                            \\right) \\in\\mathbb{R}^{4}\n                          $$`\n                          }}}\n                  )\n            ); rows\n      )\n\n(--*--)\n\n(page (:h1 Summary)\n\n      #md {{{\n             ## Autoencoder\n\n             - Works on a collection of high dimensional vectors.\n             - Does not require expected _output_.\n             - Performs automatic embedding to a lower dimensional space.\n             }}}\n\n      #md {{{\n             ## Embedding layers\n\n             - Works with general finite categorical domains\n             - Can embed into arbitrary dimensions\n             - Must be part of a larger network so the embedding\n             layer can be trained\n             }}}\n      ); page\n", "istop": false, "path": "embedding"}, "func-vector-space": {"children": [], "content": "{:title \"Functions in Vector Spaces and Applications\"\n :rank 5\n :summary #md {{{\n                 Functions in vector spaces are useful\n                 to build complex computational tasks\n                 when data is represented as tensors.\n                 }}}\n }\n\n(page (:h1 \"Functions and parameters\")\n\n      (rows :sep 4 8\n            (:h2 \"General function\")\n            #md {{{\n                   Generally speaking a _function_ is any mapping between tensors to tensors.\n\n                   For instance,\n                   $$ f : \\mathbf{R}^{n} \\to \\mathbf{R}^n $$\n                   }}}\n\n            (:h2 \"Rotation\")\n            #md {{{\n                   Consider the rotation function, mapping a 2D vector by some angle.\n\n                   $$f :\\mathbf{R}^2\\times \\mathbf{R} \\to\\mathbf{R}^2 $$\n\n                   ```\n                   $$r(v, \\theta) = \\left[\\begin{array}{cc}\n                                          \\cos(\\theta) \u0026 \\sin(\\theta) \\\\\n                                          -\\sin(\\theta) \u0026 \\cos(\\theta)\n                                          \\end{array}\\right] v$$\n                   ```\n                   }}}\n\n\n            (:h2 \"Parameterized functions\")\n            (:div #md {{{\n                         Sometimes, we want to represent a whole (infinite) set of functions by\n                         a single parameterized function.\n\n                         $$r_\\theta : \\mathbf{R}^2\\to\\mathbf{R}^2 $$\n                         }}}\n                  #md {{{\n                         We think of the angle of rotation as a _parameter_.\n\n                         What\u0027s the difference between parameters and inputs?\n                         }}}\n                  #md {{{\n                         There are several ways we can represent parameterized functions.\n\n                         $$ f_\\theta(x) $$\n                         $$ f(x | \\theta)$$\n\n                         Sometimes, when the parameter is obvious, we might not even mention it:\n\n                         $$ f(x) $$\n                         }}}\n                  )\n            )\n      )\n\n(page (:h1 \"Inference vs Parameter Estimation vs Learning\")\n\n      (rows 4 8\n\n            (:h2 \"Batch processing\")\n\n            (:div #md {{{\n                         We know that a batch of tensors is also a tensor (of one higher rank).\n                         So, if we have $f:\\mathbf{R}^n \\to \\mathbf{R}^k$, we can extend $f$ to take on a batch\n                         of vectors $X\\in\\mathbf{R}^{m\\times n}$ at a time.\n\n                         ```\n                         $$ f(X|\\theta) = \\left[\\begin{array}{c}\n                                         f(\\mathbf{x}_1|\\theta) \\\\\n                                         f(\\mathbf{x}_2|\\theta) \\\\\n                                         \\vdots\n                                         \\end{array}\\right] = Y$$\n                         ```\n                         }}}\n\n                  #md {{{\n                         We almost always work with batches:\n\n                         - $X$: batch of inputs.\n                         - $Y$: batch of outputs computed by $f$ using paramter $\\theta$.\n                         }}}\n                  )\n\n            (:h2 \"Evaluation\")\n            (:div #md {{{\n                         When we have a batch of inputs, we want to evaluate the outputs with $f$ and $\\theta$.\n\n                         _Evaluation_ refer to the computation of obtaining $Y = f(X|\\theta)$.\n\n                         $$\\mathrm{eval}: (X, \\theta, f) \\mapsto Y $$\n                         }}}\n                  #md {{{\n                         Other names for evaluation are:\n\n                         - prediction: this is when the output $Y$ is a guess of some truth.\n                         - inference: this is when the output $Y$ is a probability of the guess.\n                         }}})\n\n            (:h2 \"Parameter Estimation\")\n            (:div #md {{{\n                         We often have $f$ and $(X, Y)$, but not the parameter.  _Parameter estimation_\n                         is the process of obtaining a parameter that works well for the observed input-outputs $(X, Y)$.\n\n                         $$\\mathrm{estimate}: (X, Y, f) \\mapsto \\theta$$\n                         }}}\n                  #md {{{\n                         Other names for estimation:\n\n                         - Model identification: we think of $f(\\circ|\\theta)$ as a model of a system.\n                         - Machine learning: we think of $(X, Y)$ as training data.\n                         }}}\n                  (box #md {{{\n                              **Parameter quality**:\n\n                              How do we justify that one choice of parameter is better than another?\n                              The answer is that we need a loss function.\n                              }}})\n                  )\n\n            (:h2 \"Loss function\")\n            (:div #md {{{\n                         Let\u0027s rewrite the batch based function:\n\n                         $$ f_\\theta : \\mathrm{dom}(f) \\to \\mathrm{range}(f) : X \\mapsto Y $$\n\n                         where both $\\mathrm{dom}(f)$ and $\\mathrm{range}(f)$ are some vector spaces.\n                         }}}\n\n                  #md {{{\n                         A loss function $L$ for $f$ is a function of the signature:\n\n                         $$ L: \\mathrm{range}(f) \\times \\mathrm{range}(f)\\to\\mathbf{R} $$\n                         }}}\n\n                  #md {{{\n                         A loss function measures the amount of dissimilarity between two\n                         vectors (or tensors) in the output vector space.\n                         }}}\n\n                  #md {{{\n                         Having a loss function allows as to define the objective of\n                         learning.\n                         \n                         ```\n                         $$\\theta^*_L = \\underset{\\theta}{\\mathrm{argmin}}\\ L(Y, f(X|\\theta))$$\n                         ```\n                         }}}\n                  )\n            )\n      )\n\n(page (:h1 \"Specifics\")\n\n      #md {{{\n             We are going to survey a number of specific learning based problems, and describe them\n             as functions in vector spaces with loss functions.\n             }}}\n\n      (rows 4 8\n\n            (:h2 \"Linear regression\")\n            #md {{{\n                   A very classically popular model to describe relationships between observations of\n                   real-valued quantities such as the economic indicators and housing price.\n                   }}}\n\n            (:h2 \"Logistic regression\")\n            #md {{{\n                   Another classical model to describe relationships between real-valued quantities and\n                   a binary variable.  An example is the relationship between medical test results to the\n                   diagnosis of disease.\n                   }}}\n\n            (:h2 \"Multi-label classification\")\n            #md {{{\n                   This is a general class of problems in which we are trying to\n                   label vectors by $n$ different labels.\n                   }}}\n            )\n      )\n\n(page (:h1 \"Linear Regression\")\n\n      (rows 4 8\n\n            (:h2 \"Model\")\n\n            #md {{{\n                   The function maps a vector to a vector.\n\n                   $$ f: \\mathbf{R}^n \\to\\mathbf{R}^k $$\n\n                   $$ f(\\mathbf{x}) = A\\mathbf{x} + \\mathbf{b}$$\n                   }}}\n\n            (:h2 \"Parameter\")\n            #md {{{\n                   The model parameter is:\n\n                   $$\\theta = (A, \\mathbf{b})$$\n                   }}}\n\n            (:h2 \"Loss function\")\n            (:div #md {{{\n                         If the model is performing well, then the prediction should\n                         be very close to the expected output.  Thus, distance functions are\n                         natural choices for the loss function:\n\n                         ```\n                         $$\\newcommand{\\true}{\\mathrm{true}}\n                         \\newcommand{\\pred}{\\mathrm{pred}}\n                         L(\\mathbf{y}_\\true, \\mathbf{y}_\\pred) = \\|\\mathbf{y}_\\true - \\mathbf{y}_\\pred\\|$$\n                         ```\n                         }}}\n\n                  #md {{{\n                         __Loss function on batches__\n\n                         We can extend the loss function to batch of outputs.\n\n                         Given batch of true outputs and predictions:\n\n                         ```\n                         $$\n                         Y_\\true = \\left[\\begin{array}{c}\n                                         \\mathrm{y}^\\true_1 \\\\\n                                         \\mathrm{y}^\\true_2 \\\\\n                                         \\vdots\n                                         \\mathrm{y}^\\true_m \\\\\n                                         \\end{array}\\right]\n                         \\quad\n                         Y_\\pred = \\left[\\begin{array}{c}\n                                         \\mathrm{y}^\\pred_1 \\\\\n                                         \\mathrm{y}^\\pred_2 \\\\\n                                         \\vdots\n                                         \\mathrm{y}^\\pred_m \\\\\n                                         \\end{array}\\right]\n                         $$\n                         ```\n\n                         We can use the mean loss function:\n                         ```\n                         $$L(Y_\\true, Y_\\pred) = \\frac 1 m \\sum_i L(\\mathbf{y}^\\true_i, \\mathbf{y}^\\pred_i)$$\n                         ```\n                         }}}\n                  )\n            )\n      )\n\n(page (:h1 \"Logistic Regression\")\n\n      (rows 4 8\n            (:h2 \"The model\")\n            (:div #md {{{\n                         The input of logistic regression is the same as linear regression: vectors.\n\n                         But the output of logistic regression is a _soft binary_ `$\\mathrm{output}\\in\\{0, 1\\}$`.\n                         }}}\n\n                  #md {{{\n                         Given an input vector $\\mathbf{x}\\in\\mathbf{R}^n$, we want use a hyperplane as a separator\n                         of wheather $f(x)$ should be 0 or 1.\n\n                         Recall a n-dimensional hyperplane is defined by its normal vector $\\mathbf{a}\\in\\mathrm{R}^n$.\n                         The _distance_ between $\\mathbf{x}$ the hyperplane is given by:\n\n                         $$\\mathrm{distance} = \\mathbf{a}^T\\mathbf{x} + b$$\n\n                         But the distance can be _any_ value:\n\n                         $$-\\infty \u003c \\mathrm{distance} \u003c \\infty$$\n                         }}}\n\n                  #md {{{\n                         The sigmoid function:\n\n                         $$\\mathrm{sigmoid} :\\mathbf{R} \\to[0, 1]$$\n                         }}}\n                  (image \"sigmoid.png\"\n                         #md {{{\n                                Reference: https://www.researchgate.net/figure/An-illustration-of-the-signal-processing-in-a-sigmoid-function_fig2_239269767\n                                }}})\n\n                  #md {{{\n                         **The logistic model**\n\n                         $$f(\\mathbf{x}) = \\frac{1}{1 + \\exp(-\\mathbf{a}^T\\mathbf{x} + b)}$$\n                         }}}\n                  )\n\n            (:h2 \"The parameter\")\n            #md {{{\n                   The parameter is the normal vector of the hyperplane and the offset.\n\n                   $$\\theta = (\\mathbf{a}, b)$$\n                   }}}\n\n            (:h2 \"The loss function\")\n            #md {{{\n                   The logistic regression model is a probabilistic model: its output is a _probability_.\n\n                   So, the loss function will need to measure the dissimilarity between a true\n                   observation and a probability distribution of the observation.\n                   }}}\n            )\n      )\n\n(page (:h1 \"Probability vs Observation\")\n\n      (rows 4 8\n            (:h2 \"Objective\")\n            #md {{{\n                   We have to measure the dissimilarity between true observations\n                   `$y_\\true\\in\\{0, 1\\}$` and the probabilistic guess\n                   of the model `$p_\\pred$`.\n\n                   - Low dissimilarity: \n                      - `$y_\\true = 0$` and `$p_\\pred\\simeq 0$`.\n                      - `$y_\\true = 1$` and `$p_\\pred\\simeq 1$`.\n                   - High dissimilarity:\n                      - `$y_\\true = 0$` and `$p_\\pred \\simeq 1$`\n                      - `$y_\\true = 1$` and `$p_\\pred\\simeq 0$`.\n                   }}}\n\n            (:h2 \"Loss function for logistic regression\")\n            #md {{{\n                   So, we want:\n\n                   \u003e ```\n                   \u003e def dissimilarity($y_\\true$, $p_\\pred$):\n                   \u003e     if $y_\\true = 0$:\n                   \u003e         return $\\log (1-p_\\pred)$\n                   \u003e     else:\n                   \u003e         return $\\log p_\\pred$\n                   \u003e ```\n\n                   This is exact captured by:\n\n                   ```\n                   $$ L(y_\\true, p_\\pred) = -\\big[y_\\true\\log p_\\pred + (1-y_\\true)\\log(1-p_\\pred)\\big] $$\n                   ```\n\n                   This is known as the _binary cross-entropy_ between `$y_\\true$` and `$p_\\pred$`.\n                   }}}\n            )\n      )\n", "istop": false, "path": "func-vector-space"}, "intro": {"children": [], "content": "{:title \"Introduction\"\n :rank 1\n :summary #md {{{\n                 A brief overview of the course and its prerequisites.\n                 }}}\n }\n\n(page (:h1 \"Syllabus\")\n\n      #md {{{\n             https://docs.google.com/document/d/1CIJZq7bBPj5OQIAEzlAjCthuoN_kOyC1arSPYcPBHdU/edit?usp=sharing\n             }}}\n      )\n\n(page (:h1 \"Why this course is important?\")\n\n      (rows :sep 4 8\n            (:h2 \"To ask or not to ask?\")\n            #md {{{\n                   \u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/36GT2zI8lVA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n                   }}}\n\n            (:h2 \"Short answer\")\n            #md {{{\n                   - Foundation to _all_ software of the future.\n                      \u003e Computers are no longer looking at the world as binary strings,\n                      \u003e but rather vectors in strange manifolds.\n\n                   - Just like _compilers_, we will more likely to be the power users of deep learning libaries.\n                      \u003e Not understanding the theories of machine learning will make using Tensorflow (or Pytorch) quite impossible.\n                   }}}\n\n            (:h2 \"By the of the course...\")\n\n            #md {{{\n                   You will be able to formulate the right _why_ questions.\n\n                   - Which jobs will disappear due to machine learning?\n\n                   - What are the transformational changes resulting from machine learning?\n\n                   - Is machine learning good or evil or both?\n\n                   - What are the social impact and ethical considerations of _large_ machine learning deployments?\n                   }}}\n\n            (:h2 \"The 10 minutes long answer\")\n            #md {{{\n                   \u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/sqRZGz32c6Y?start=75\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n                   }}}\n            )\n      )\n\n(page (:h1 \"Mathematical requirements\")\n\n      (rows 6 6\n            #md {{{\n                   _Linear algebra_\n\n                   \u003e You absolutely need some level of linear algebra.\n                   \u003e\n                   \u003e We will review the basic elements, but you should be familiar with\n                   the definitions of:\n                   \u003e\n                   \u003e - Vectors and matrices\n                   \u003e - Vector space and basis\n                   \u003e - Transformations.\n                   }}}\n\n            #md {{{\n                   _Calculus_\n\n                   \u003e You absolutely need some level of calculus.\n                   \u003e\n                   \u003e We will be dealing with many continuous functions, and refer\n                   to their properties.  You should already know:\n                   \u003e\n                   \u003e - Derivatives\n                   \u003e - Gradients\n                   \u003e - Multivariant calculus (over multiple variables)\n                   }}}\n\n            #md {{{\n                   _Probability_\n\n                   \u003e Machine learning is a close cousin of statistical learning, which\n                   \u003e is almost entirely built on top of probability.  A solid understanding\n                   \u003e of probability will make deep learning _much easier_.\n                   \u003e\n                   \u003e You should already know:\n                   \u003e\n                   \u003e - Random variables (continuous and discret)\n                   \u003e - Probability distributions and their parameters\n                   \u003e - Sampling\n                   \u003e - Inference and confidence intervals\n                   }}}\n            )\n      )\n\n(page (:h1 \"Technical requirements\")\n\n      (rows 6 6\n            #md {{{\n                   _Python_\n\n                   \u003e All the code will be developed in Python.  You only need a\n                   basic understanding of Python for the _programming_ needs.\n                   However, we will utilize a number of python libraries extensively.\n\n                   \u003e - Numpy\n                   \u003e - Pandas\n                   \u003e - Matplotlib\n                   }}}\n\n            #md {{{\n                   _Jupyter Notebook_\n\n                   \u003e All development will be done as Python Jupyter notebooks.  A clear understanding\n                   of how Jupyter notebook evaluates Python code will definitely be helpful.\n                   }}}\n            )\n      )\n\n\n\n(page (:h1 \"Learning environment\")\n\n      (rows 6 6\n            #md {{{\n                   _Github Classroom_\n\n                   \u003e We use [Github Classroom](https://classroom.github.com/).\n                   \u003e This means:\n                   \u003e\n                   \u003e - You _absolutely_ need to have a Github account that is\n                   \u003e associated with your _ontariotechu.net_ email.\n                   \u003e\n                   \u003e - You will benefit from knowing `git` well.\n                   }}}\n\n            #md {{{\n                   _Google Colaboratory_\n\n                   \u003e https://colab.research.google.com\n                   \u003e\n                   \u003e - Is free.\n                   \u003e - Supports integration with Github (as needed for github classroom).\n                   \u003e - You can utilize your own Jupyter notebook installation (with Tensorflow 2.x).\n                   }}}\n            )\n      )\n", "istop": false, "path": "intro"}, "kernel-methods": {"children": [], "content": "{:title \"Learning Using Kernels\"\n :rank 13\n :author #md {{{\n                - Ken Pu\n                - Faculty of Science, UOIT\n                - Copyright 2017 }}}\n :summary #md {{{\n                 Kernel methods are powerful techniques\n                 that convert simple and limited learning\n                 algorithms (e.g. perceptron) into supercharged\n                 learning machines.  The power comes from\n                 feature-space transformations.\n\n                 We will cover support vector machines (SVM)\n                 and radical basis function networks (RBFN)\n                 both of which make good usage of kernel\n                 methods.\n                 }}}\n }\n\n(:h1 \"Feature Space Transformation\")\n\n(page (:h1 \"Linear Separability\")\n\n      #md {{{\n             Let\u0027s review a single neuron.\n\n             `$$ y = \\phi(\\mathbf{w}^T\\mathbf{x} + b) $$`\n\n             where both $\\mathbf{w}$ and $\\mathbf{x}$ are\n             in the input space $\\mathbb{R}^m$.\n             }}}\n\n      (rows :sep 6 6\n            #md {{{\n                   What if it operates on\n                   the following binary classification\n                   problem?\n\n                   \u003e Perceptron can only perform _linear separation_, so it will\n                   fail quite badly.\n                   }}}\n            (image \"nosep.png\"\n                   #md \"http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html\"))\n\n      #md {{{\n             ## Feature space transformation\n\n             But, what if we can transform the input space from\n             a simple 2D two a higher dimension space?\n\n             `$$\n             h: \n             \\left[\\begin{array}{c}\n                   x \\\\\n                   y\n                   \\end{array}\\right]\n             \\mapsto\n             \\left[\\begin{array}{c}\n                   x \\\\\n                   y \\\\\n                   x^2 + y^2\n                   \\end{array}\\right]\n             $$`\n             }}}\n      (image \"mapped.png\"\n             #md \"http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html\")\n\n      #md {{{\n             Now, we can rewrite the perceptron using the transformation:\n\n             $$y = \\phi(\\mathbf{w}^T h(\\mathbf{x}) + b) $$\n             }}}\n\n      (image \"hyperplane.png\")\n      ); page\n\n(:h1 \"Support Vector Machines\")\n\n(page (:h1 \"Linear separation and its margin\") \n      #md {{{ \n             We know that both perceptron with hard limit activation\n             and logistic neuron with the sigmoid activation\n             can separate linearly separable training data.\n             }}}\n\n      #md {{{\n             But how _safe_ is the separation?\n\n             \u003e There is no guarantee.\n             }}}\n\n      (rows :sep 5 7\n            (image \"svm-margin.png\"\n                   #md {{{\n                          A separation with maximal separating margin.\n\n                          https://en.wikipedia.org/wiki/Support_vector_machine\n                          }}})\n            #md {{{\n                   Suppose that the training data is given as:\n\n                   `$$T = \\{(\\mathbf{x}_i, y_i): i=1,2,\\dots, n\\}$$`\n\n                   where `$y_i \\in \\{-1, 1\\}$`, and there are $n$ samples.\n\n                   The separating hyperplane is given by:\n\n                   `$$\\mathbf{w}^T\\mathbf{x} + b = 0$$`\n                   }}}\n\n\n            (:h1 \"Margin\")\n\n            #md {{{\n                   The margin is defined as the distance between\n                   the two lines:\n\n                   `$$\\mathbf{w}^T\\mathbf{x} + b = -1$$`\n                    and\n                   `$$\\mathbf{w}^T\\mathbf{x} + b = 1$$`\n\n                   That distance given by:\n\n                   $$ d = \\frac{2}{\\|\\mathbf{w}\\|} $$\n\n                   The vector `$\\mathbf{w}$` is choosen so\n                   that no `$\\mathbf{x}_i$` is in the margin.\n                   }}}\n\n            (:h1 \"Maximal margin separation\")\n\n            #md {{{\n                   To create the maximal margin, we want\n                   to _minimize_ `$\\|\\mathbf w\\|$` while maximally separating\n                   the training data.\n\n                   This can be done by the following optimization:\n\n                   `$$\\mathrm{cost}(\\mathbf{w}, b|T)\n                     = \\left[\\frac{1}{n}\\sum_{i=1}^n\\max(0, 1-y_i(\\mathbf{w}^T\\mathbf{x}_i+b))\\right]\n                       +\\lambda \\|\\mathbf{w}\\|^2\n                       $$`\n                   }}}\n            ); rows\n      ); page\n\n(page (:h1 \"SVM with Kernel Function\")\n      (row 6 6 \n           #md {{{\n                  The classification power of SVM is fully manifested\n                  when the maximal margin linear classification is\n                  combined with a non-linear kernel function such\n                  as the polynomial kernel.\n                  }}}\n\n           (image \"kernel-svm.png\"\n                  #md {{{\n                         https://en.wikipedia.org/wiki/Support_vector_machine\n                         }}}))\n\n       (:h2 \"Cost function using the kernel function\")\n\n       #md {{{\n              `$$\\mathrm{cost}(\\mathbf{w}, b|T)\n                 = \\left[\\frac{1}{n}\\sum_{i=1}^n\n                         \\max(0, 1-y_i((\\mathbf{w}^T\\mathbf{x}_i+1)^d+b))\\right]\n                   +\\lambda \\|\\mathbf{w}\\|^2\n                   $$`\n              }}}\n      ); page\n\n(:h1 \"Radial Basis Functions\")\n\n(page (:h1 \"Radial Basis Functions\")\n\n      #md {{{\n             Consider an input space $\\mathbb{R}^n$.\n\n             Suppose that we have a center $\\mathbf{c}$,\n             and given an input vector $\\mathbf{x}$,\n             define:\n\n             `$$\\rho(\\|\\mathbf{x}-\\mathbf{c}\\|)\n             = \\exp\\left(-\\frac{\\|\\mathbf{x}-\\mathbf{c}\\|^2}{2\\sigma}\\right)\n             $$`\n             }}}\n\n      #md {{{\n             $\\rho(\\|\\mathbf{x}-\\mathbf{c}\\|)$ measures the distance\n             the input $\\mathbf{x}$ is away from the center $\\mathbf{c}$\n             subject to an exponential delaying rate of $1/2\\sigma$.\n             }}})\n\n(page (:h1 \"Transformation to $k$-dimensions\")\n\n      #md {{{\n             If we have $k$ centers $\\mathbf{c}_i$\n             and decay rates $\\sigma_i$, where $i=1,2,\\dots,k$, \n             we can transform\n             the input vector $\\mathbf{x}$ form $\\mathbb{R}^n$\n             to a different dimension $\\mathbb{R}^k$.\n             }}}\n\n      #md {{{\n             Given the centers $C=\\{\\mathbf{c}_1, \\dots, \\mathbf{c}_k\\}$\n             and the delays factors $\\sigma = (\\sigma_1, \\dots, \\sigma_k)$,\n\n             `$$ \\phi(\\mathbf{x}|C,\\sigma) = \\left[\\begin{array}{c}\n                                          \\rho_1(\\|\\mathbf{x}-\\mathbf{c}_1\\|) \\\\\n                                          \\vdots \\\\\n                                          \\rho_k(\\|\\mathbf{x}-\\mathbf{c}_k\\|)\n                                          \\end{array}\\right] \\in\\mathbb{R}^k\n             $$`\n             }}}\n\n      #md {{{\n             The hope is that if $C, \\sigma$ are chosen properly, then\n             in the transformed space, $\\mathbb{R}^k$, the feature\n             vectors can be easily separated using a simple linear classifier,\n             such as a perceptron.\n             }}}\n      ); page\n\n(page (:h1 \"Radial Basis Function Network\")\n\n      (row 6 6 \n            #md {{{\n                   RBF Network works by cascading $\\phi(\\mathbf{x}|C, \\sigma)$\n                   with a linear neuron.  For binary classification, the neuron would\n                   have an sigmoid activation function:\n\n                   `$$y = \\mathrm{sigmoid}(W^T\\phi(\\mathbf{x}|C, \\sigma) + b)$$`\n                   }}}\n            (image \"rbfn.png\"))\n\n      (:h2 \"Training\")\n      \n      #md {{{\n             Unlike the polynomial kernel, the transformation by RBF can be _trained_ to maximally\n             make the features linearly separable.\n             }}}\n\n      #md {{{\n             Given a training data $T$, the learning algorithm can optimize over the centers and decay rates\n             of the radical basis functions:\n\n             `$$\\underset{C, \\sigma, W, b}{\\mathrm{argmin}}\\big[ \\mathrm{loss}(y_\\mathrm{true}, y\\ |\\ C,\\sigma,W, b) \\big]$$`\n             \n             where \n             - `$C\\in\\mathbb{R}^{n\\times k}$`,\n             - `$\\sigma\\in\\mathbb{R}^k$`\n             - `$W\\in\\mathbb{R}^{k}$`\n             - `$b\\in\\mathbb{R}$`\n             }}}\n\n      (row 6 6\n           #md {{{\n                  Here is an example of RBF network in action.\n\n                  Note that the points are not linearly separable in 2D, but\n                  the RBF network can express the model using\n                  non-linearity.\n                  }}}\n           (image \"sep-surface.png\"))\n      ); page\n\n(page (:h1 \"Implementing the RBF layer\")\n\n      #md {{{\n             Keras does not come with a RBF layer.  So, it\u0027s a good\n             exercise to see how one can implement custom\n             keras layers using basic TensorFlow API.\n             }}}\n\n      )\n\n(page (:h1 \"Transforming tensors\")\n\n      (rows 4 8\n            (:h2 \"Input-output\")\n            #md {{{\n                   The input tensor `x` is of shape `(N, 2)`, which means\n                   it\u0027s a batch of two dimensional vectors.  The batch size\n                   is `N`.\n\n                   The output tensor `y` is of shape `(N, k)` where `k`\n                   is the number of clusters.\n                   }}}\n\n            (:h2 \"Model parameters\")\n            #md {{{\n                   The model parameters are:\n\n                   - `c`: the centers of the `k` clusters.  `c` is of the shape `(k,2)`.\n                   - `b`: the decay rate of each cluster.  `b` is of the shape `(k,)`.\n                   }}}\n\n            (:h2 \"Transformation\")\n            #md {{{\n                   We will use a _trick_ called _broadcasting_.\n\n                   ```\n                   def rbf(x, c, b)\n                     # computes the pair-wise distance between\n                     # the N input points and k centers\n                     # d: (N, k, 2)\n\n                     d = tf.reshape(x, (-1, 1, 2)) - tf.reshape(c, (1, -1, 2))\n\n                     # dist: (N, k)\n                     dist = tf.pow(tf.norm(d, axis=2), 2)\n\n                     # dist * decay : (N, k)\n                     norm_dist = dist * tf.reshape(b, (1, -1))\n\n                     # the final output: (N, k)\n                     y = tf.exp(norm_dist)\n\n                     return y\n                   ```\n                   }}}\n            )\n      )\n\n(page (:h1 \"Custom Keras Layer\")\n\n      (rows 4 8\n            (:h2 \"Custom layer\")\n            #md {{{\n                   Keras supports custom layer.  A custom layer\n                   is a Python class:\n\n                   ```\n                   class CustomLayer(tensorflow.keras.layers.Layer):\n                       def __init__(self, ...):\n                          # initialize\n                       \n                       def call(self, x):\n                          # x is a batch tensor\n                          # perform the transformation\n                          # to output y\n\n                          return y\n                   ```\n                   }}}\n            (:h2 \"The RBF custom layer\")\n            (:div (code :python\n                        {{{\n                           # begin of RBF\n                           class RBF(Layer):\n                               def __init__(self, k, **kw_args):\n                                  super(RBF, self).__init__(**kw_args)\n                                  self.c = tf.Variable(np.random.randn(k, 2))\n                                  self.b = tf.Variable(np.random.randn(k))\n                           }}})\n                  (code :python\n                        {{{\n                               def call(self, x):\n                                  y = rbf(x, self.c, self.b)\n                                  return y\n                           # end of RBF\n                           }}}))\n\n            (:h2 \"Using the RBF layer\")\n            (:div (code :python\n                        {{{\n                           model = Sequential([\n                               Input(shape=(2,)),\n                               RBF(5),\n                               Dense(2, activation=\u0027softmax\u0027)\n                           ])\n                           model.fit(x_train, y_train, epochs=10)\n                           }}}\n                        )\n                  )\n            )\n      )\n\n(page (:h1 \"Summary\")\n\n      #md {{{\n             - Kernel methods are powerful techniques to separate\n             low dimensional points.\n\n             - Radial basis functions are effective in generating\n             non-linear separation boundaries.\n\n             - Keras is an extensible framework. We can author custom\n             layers that can be combined with existing Keras layers\n             seamlessly.\n             }}}\n      )\n", "istop": false, "path": "kernel-methods"}, "neural-networks": {"children": [], "content": "{:title \"Introduction to Neural Networks\"\n :rank 9\n :summary #md {{{\n                 We define neural networks as a unifying framework\n                 to describe non-linear functions over tensors.\n                 Many problems we have encountered can be expressed\n                 as neural networks.\n                 }}}}\n\n(page (:h1 \"A neuron\")\n\n      #md {{{\n             In this course, we use the term _neuron_ as a mathematical\n             object that describes a _simple_ non-linear function.\n             }}}\n\n      (rows 4 8\n            (:h2 \"What is a neuron?\")\n            #md {{{\n                   A neuron maps a vector $x\\in\\mathbf{R}^n$\n                   to a _scalar_ output $y\\in\\mathbb{R}$ as follows:\n                   \n                   ```\n                   $$ y = \\phi(\\left\u003cw, x\\right\u003e + b) $$\n                   ```\n\n                   The model parameter is $\\mathrm{variables}(\\mathrm{neuron}) = (w, b)$.\n                   }}}\n\n            (:h2 \"Weight vector\")\n\n            #md {{{\n                   The model parameter $w\\in\\mathbb{R}^n$ is called the weight vector, or\n                   just the weights of the neuron.\n                   }}}\n\n            (:h2 \"Bias\")\n\n            #md {{{\n                   The bias is the scalar $b\\in\\mathbb{R}$.\n                   }}}\n\n            (:h2 \"Activation function\")\n\n            #md {{{\n                   The output is computed by a function $\\phi :\\mathbb{R}\\to\\mathbb{R}$.\n\n                   The activation function is _fixed_ for the neuron, and is not\n                   part of the model parameter.\n                   }}}\n            (:h2 \"A diagram\")\n            (image {:style {:width 300}} \"feedforward.jpg\" \n                   #md {{{ \n                          Reference: [_Neural Networks_](https://books.google.ca/books?id=K7P36lKzI_QC\u0026q=Neural+Networks+and+Learning+Machines\u0026dq=Neural+Networks+and+Learning+Machines\u0026hl=en\u0026sa=X\u0026ved=0ahUKEwiB8PSry6flAhWsTt8KHZztBhEQ6AEIKDAA) by Simon Haykin.\n                          }}})\n            )\n      )\n\n(page (:h1 \"Learning using a neuron\")\n\n      (rows 4 8\n            (:h2 \"Learning\")\n            #md {{{\n                   If we have observations of input-output data, we can train a neuron\n                   to produce a good approximation of the desired output.\n                   }}}\n\n            (:h2 \"Training data\")\n            #md {{{\n                   A single observation is given by $(x, y)$.\n\n                   A collection of $m$ observations is given by $(X, Y)$,\n                   where\n\n                   - $X\\sim(m, n)$\n                   - $Y\\sim(m)$\n                   }}}\n            (:h2 \"Loss function\")\n            #md {{{\n                   The distance function:\n\n                   `$$\n                   L(y_\\mathrm{true}, y_\\mathrm{pred}) = \\|y_\\mathrm{true} - y_\\mathrm{pred}\\|^2 \n                   $$`\n                   }}}\n\n            (:h2 \"Training\")\n            #md {{{\n                   The training is to optimize $(w, b)$ to minimize the\n                   loss function:\n\n                   `$$ (w^*, b^*) = \\mathrm{argmin}_{w,b}\\sum_i L(y_i, \\phi(w^Tx_i + b)) \n                   $$`\n                   }}}\n            )\n      )\n\n(page (:h1 \"Application of neuron\")\n\n      (rows 4 8\n            (:h2 \"Linear regression\")\n            (:div #md {{{\n                         Consider the problem of linear regression:\n\n                         $$y \\sim ax + b$$\n\n                         where we want to estimate $(a, b)$ with respect to\n                         some observation $(X, Y)$.\n                         }}}\n                  #md {{{\n                         We can frame this as a neuron learning problem.\n\n                         - Activation: $\\phi(y) = y$\n                         - Neuron: $y = \\phi(w^Tx + b) = w^T x + b$\n                         - Loss: distance function $L(y_1, y_2) = \\|y_1-y_2\\|^2$\n                         }}}\n                  )\n\n            (:h2 \"Logistic regression\")\n            (:div #md {{{\n                         - Activation $\\phi = \\mathrm{sigmoid}(y)$\n                         - Neuron: $y = \\mathrm{sigmoid}(w^T x = b)$\n                         - Loss: binary cross entropy function $L(y, p) = -(y\\log(p) + (1-y)\\log(1-p))$\n                         }}}\n                  )\n            )\n      )\n\n(page (:h1 \"A layer\")\n\n      (rows 4 8\n            (:h2 \"Limitation of a neuron\")\n            #md {{{\n                   Each neuron can only generate _one_ scalar.  We want to output\n                   vectors in general.\n                   }}}\n\n            (:h2 \"Learning vector functions\")\n            (:div #md {{{\n                         How can we learn a function over vectors:\n\n                         $$ f:\\mathbb{R}^n \\to\\mathbb{R}^k $$\n                         }}}\n                  (box #md \"**Use $k$ neurons.**\"))\n\n            (:h2 \"Diagram\")\n            (image {:style {:width 300}}\n                   \"multi-feedforward.png\"\n                   #md {{{ \n                          Reference: [_Neural Networks_](https://books.google.ca/books?id=K7P36lKzI_QC\u0026q=Neural+Networks+and+Learning+Machines\u0026dq=Neural+Networks+and+Learning+Machines\u0026hl=en\u0026sa=X\u0026ved=0ahUKEwiB8PSry6flAhWsTt8KHZztBhEQ6AEIKDAA) by Simon Haykin.\n                          }}})\n\n            (:h2 \"Extended model\")\n\n            #md {{{\n                   The extended model is:\n\n                   $$y = \\phi(Wx + b)$$\n\n                   where:\n                   - $W\\sim(k,n)$ is called the **weight matrix**\n                   - $b\\sim(k)$ is called the **bias vector**\n                   - $y\\sim(k)$ is the **output**.\n                   }}}\n            )\n      #md {{{\n             Let\u0027s temporally called the single layer by the function: \n             $\\mathbf{N}:\\mathbb{R}^n\\to\\mathbb{R}^k$,\n             and the model parameter of $\\mathbf{N}$ is \n             \n             $$\\mathrm{variables}(\\mathbf{N}) = (W, b)$$\n             }}}\n\n      (note #md {{{\n                   The layer we described here the simplest kind, commonly\n                   known as the _dense_ layer.\n                   }}})\n      )\n\n(page (:h1 \"Multi-layer perceptron (MLP)\")\n      #md {{{\n             A layer is a function that maps vectors to vectors.  So we can concatenate\n             multiple layers together.\n             }}}\n\n      (rows 4 8\n            (:h2 \"Organization\")\n            (image {:style {:width 600}}\n                   \"mlp.jpg\"\n                    #md {{{ Reference: [_Neural Networks_](https://books.google.ca/books?id=K7P36lKzI_QC\u0026q=Neural+Networks+and+Learning+Machines\u0026dq=Neural+Networks+and+Learning+Machines\u0026hl=en\u0026sa=X\u0026ved=0ahUKEwiB8PSry6flAhWsTt8KHZztBhEQ6AEIKDAA) by Simon Haykin.\n                                }}})\n\n            (:h2 \"Mathematical definition\")\n\n            (:div #md {{{\n                         **Input**\n\n                         \u003e The input to MLP is vectors from $\\mathrm{R}^n$.\n                         }}}\n                  #md {{{\n                         **Layers**\n\n                         \u003e A MLP consists of multiple layers:\n                         \u003e\n                         \u003e $(\\mathbf{N}_1, \\mathbf{N}_2, \\dots, \\mathbf{N}_M)$\n                         \u003e\n                         \u003e where each layer $\\mathbf{N}_i$ comes with its\n                         \u003e own weight matrix, bias vector **and** the \n                         \u003e \u003cspan style=color:red\u003e_activation function_\u003c/span\u003e.\n                         }}}\n                  #md {{{\n                         **Model Parameter**\n\n                         \u003e The model parameter of a MLP is all the model parameters of\n                         each of the layers.\n                         \u003e\n                         \u003e $\\mathrm{variables}(\\mathbf{MLP}) = (W_1, b_1, W_2, b_2, \\dots, W_M, b_M)$\n                         }}}\n                  #md {{{\n                         **Output**\n\n                         \u003e `$\n                         \u003e x \\overset{\\mathbf{N}_1}{\\longrightarrow}\n                         \u003e z_1 \\overset{\\mathbf{N}_2}{\\longrightarrow}\n                         \u003e z_2 \\overset{\\mathbf{N}_3}{\\longrightarrow}\n                         \u003e \\dots \\overset{\\mathbf{N}_M}{\\longrightarrow} y\n                         \u003e $`\n                         }}}\n                  )\n\n            (:h2 \"Training\")\n\n            #md {{{\n                   **Loss function**\n\n                   \u003e Same as before.\n\n                   **Training**\n\n                   \u003e Same as before, however, we rely on a more sophisticated\n                   gradient computation called the _backpropagation algorithm_.\n                   }}}\n            )\n      )\n\n(page (:h1 \"MLP is important\")\n\n      (rows 4 8\n            (:h2 \"MLP is expensive\")\n\n            #md {{{\n                   The model complexity is the number of scalar parameters\n                   in $\\mathrm{variables}(\\mathbf{MLP})$.\n\n                   `\n                   $$\\big|\\mathrm{variables}(\\mathbf{MLP})\\big| = \\sum_{i=0}^{M-1} n_i\\cdot n_{i+1}$$\n                   `\n                   }}}\n\n            (:h2 \"MLP is flexible\")\n\n            #md {{{\n                   MLP can mix up different types of layers:\n\n                   - Different non-linear activation functions\n                   - Different dimensionality for intermediate layers\n                   }}}\n\n            (:h2 \"MLP is extensible\")\n\n            #md {{{\n                   - Layers can be added or removed to adjust the complexity\n                   of the model.\n\n                   - Two MLP can be combined.\n                   }}}\n\n            (:h2 \"MLP is universal\")\n\n            #md {{{\n                   The **universal approximation theorem** states that MLP\n                   can approximate _any_ differentiable function\n                   to arbitrary precision.\n\n                   - (1989) George Cybenko: universal approximation using _sigmoid_ activation\n                   - (1991) Kurt Hornik: universal approximation with generic activation\n                   - (2017) Lu et al: univeral approximation with _narrow_ networks using RELU activation\n                   - (2018) Hanin: more compact networks using _RELU_ activation\n\n                   https://en.wikipedia.org/wiki/Universal_approximation_theorem\n                   }}}\n            )\n      )\n\n(page (:h1 \"Activation functions\")\n\n      (rows 4 8\n            (:h2 \"What is an activation function?\")\n            #md {{{\n                   In the context of MLP, an activation function maps\n                   vector to vector.  The input and output vectors are\n                   of the same shape.\n\n                   We will define activation functions as:\n\n                   $$ y = f(x) $$\n                   }}}\n\n            (:h2 \"Linear activation\")\n            #md {{{\n                   $$ f(x) = x $$\n                   }}}\n\n            (:h2 \"Sigmoid activation\")\n            (:div #md {{{\n                         $$ f(x) = \\frac{1}{1+\\exp(x)} $$\n                         }}}\n                  (image \"sigmoid.svg\"))\n\n            (:h2 \"Hyperpolic tangent (tanh)\")\n            (:div #md {{{\n                         $$f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x+e^{-x}}$$\n                         }}}\n                  (image {:style {:width \"100%\"}} \"tanh.svg\")\n                  )\n\n            (:h2 \"Reactified linear unit (RELU)\")\n            (:div #md {{{\n                         `$$ f(x) = \\left\\{\\begin{array}{ll}\n                                         0 \u0026 \\mathrm{if}\\ x \\leq 0 \\\\\n                                         x \u0026 \\mathrm{else}\n                                         \\end{array}\\right. $$`\n                         }}}\n                  (image {:style {:width \"100%\"}} \"relu.svg\")\n                  )\n\n            (:h2 \"Softmax\")\n            (:div #md {{{\n                         Softmax maps vectors to vectors: $y = f(x)$\n\n                         `$$\n                         y_i = \\frac{\\exp(x_i)}{\\sum_{j} \\exp(x_j)}\n                         $$`\n                         }}}\n                  #md {{{\n                         If $x\\in\\mathrm{R^2}$, and $x_1 + x_2$,\n                         then $x$ is a probability distribution.\n\n                         In this case:\n\n                         $$\\mathrm{softmax}(x_1, 1-x_1)\n                         =(\\mathrm{sigmoid}(x_1), 1-\\mathrm{sigmoid}(x_1))\n                         $$\n                         }}}\n                  )\n            )\n      )\n\n(page (slide (:h1 \"Summary\")\n\n            (rows :sep 6 6\n               #md {{{\n                      Single Neuron:\n\n                      \u003e - Vector input, scalar output\n                      \u003e - Weight vector, bias scalar\n                      \u003e - Arbitrary activation function\n                      }}}\n\n               #md {{{\n                      Single Layer of neurons:\n\n                      \u003e - Vector input, vector output\n                      \u003e - Weight matrix, bias vector\n                      \u003e - Arbitrary activation including softmax\n                      \u003e - *Not universal*\n                      }}}\n\n               #md {{{\n                      Multilayer perceptron:\n\n                      \u003e - Vector input, vector output\n                      \u003e - Weight tensor, bias matrix\n                      \u003e - Arbitrary activation, different at each layer.  Softmax is often at the output layer.\n                      \u003e - *Universal*\n                      }}}\n               #md {{{\n                      Learning:\n\n                      \u003e All network architectures use the same learning strategy.\n                      }}}\n               )\n            )\n      )\n", "istop": false, "path": "neural-networks"}, "regulation": {"children": [], "content": "{:title \"Regulation\"\n :rank 12\n :summary #md {{{\n                 We discuss the importance and methods in regulating\n                 the model parameters to avoid overfitting.\n                 }}}\n }\n\n(page (:h1 \"Weight Regulation\")\n\n      (rows 4 8\n            (:h2 \"A single layer\")\n            #md {{{\n                   Consider:\n                   $ y = \\sigma(Wx + b) $\n                   }}}\n\n            (:h2 \"What\u0027s wrong with this?\")\n            (:div #md {{{\n                         $y = \\sigma(0.1x_1 + 8424.1 x_2 + 382.438)$\n                         }}}\n                  (toggle #md {{{\n                                 - Inputs are best restricted to [0, 1].\n                                 - But the weights are so large that\n                                 $wx$ becomes too large.\n                                 }}})\n                  (toggle #md {{{\n                                 - Large values causes gradient descent to become\n                                 unstable, due to large gradients.\n                                 }}})\n                  (toggle #md {{{\n                                 - Real models should not have _huge_ weights. So, large\n                                 weights may indicate that the model has\n                                 overfitted.\n                                 }}})\n                  )\n\n            (:h2 \"Key is regulation\")\n            #md {{{\n                   - We introduce new terms to the loss function\n                   to penalize _large model parameters_.\n                   }}}\n            )\n      )\n\n(page (:h1 \"Regularizer\")\n\n      (rows 4 8\n            (:h2 \"Reviewing layers\")\n            (:div #md {{{\n                         ```\n                         [Input] -\u003e [Layer 1] -\u003e [Layer 2] -\u003e ...\n                         ```\n                         }}}\n                  #md {{{\n                         Let each layer be\n                         with model parameters $\\theta_i$.\n                         }}}\n                  #md {{{\n                         Let $L_0$ be the original loss function.\n                         }}}\n                  )\n            (:h2 \"Measuring the _size_ of model parameters\")\n            (:div #md {{{\n                         - How do we measure the _size_ of the parameters $\\theta$?\n                         - Recall that $\\theta$ is just a (big) collection\n                         of scalar variables (arranged into one or more tensors).\n                         - We can view $\\theta$ as a vector.\n                         }}}\n                  #md {{{\n                         - The $\\ell_2$-norm:\n\n                            `$$\\|\\theta\\|_2 = \\sqrt{\\sum_{v\\in\\theta} |v|^2}$$`\n\n                         - The $\\ell_1$-norm:\n\n                            `$$\\|\\theta\\|_1 = \\sum_{v\\in\\theta} |v|$$`\n                         }}}\n                  #md {{{\n                         Both $\\ell_1$ and $\\ell_2$ penalizes _large_ weights.\n                         }}}\n                  )\n\n            (:h2 \"General regularizer\")\n            (:div #md {{{\n                         **A regularizer**\n\n                         \u003e A _regularizer_ is a loss function on the model parameter $\\theta$:\n                         \u003e\n                         \u003e $\\mathrm{regularizer} : \\theta\\mapsto\\mathbf{R}$.\n                         }}}\n                  )\n\n            (:h2 \"Regularized models\")\n            #md {{{\n                   ```\n                              [Layer 1    ]                 [Layer 3    ] \n                   [Input] -\u003e [regularizer] -\u003e [Layer 2] -\u003e [regularizer] -\u003e ...  \n                              [           ]                 [           ]\n                   ```\n                   }}}\n\n            (:h2 \"But what are regularizers???\")\n            (:div #md {{{\n                         Regularizers are _additional_ terms in the loss function.\n                         }}}\n                  #md {{{\n                         Let $L_0 : \\mathrm{Output}\\mapsto\\mathbf{R}$ be the original loss\n                         function.\n\n                         The regularized loss function is:\n\n                         `$$L = L_0(y_\\mathrm{true}, y_\\mathrm{pred}) + c_1\\times\\mathrm{regularizer}_1(\\theta_1) + c_2\\times\\mathrm{regularizer}_2(\\theta_2)$$`\n\n                         Here we have two regularizers on two layers $\\theta_1$ and $\\theta_2$.\n                         }}}\n                  )\n            )\n      )\n\n(page (:h1 \"Regularizing MLP in Keras - an example\")\n\n      (rows 4 8\n\n            (:h2 \"Import packages\")\n            (code :python\n                  {{{\n                     from tensorflow.keras.regularizers import l1, l2\n                     }}})\n\n            (:h2 \"Build MLP with regularizers\")\n            (code :python\n                  {{{\n                     model = Sequential([\n                       Input(...),\n                       Dense(100, kernel_regularizer=l2(0.1)),\n                       Dense(10, kernal_regularizer=l2(0.001)),\n                       ...\n                     ])\n                     }}})\n\n            (:h2 \"Another way of building regularized models\")\n            (:div #md {{{\n                         First we need to have a _loss function_\n                         factory that produces regularized\n                         loss function.\n                         }}}\n                  (code :python\n                        {{{\n                           def regulated_loss(model):\n                             c1 = 0.1\n                             c2 = 0.001\n                             def loss(y_true, y_pred):\n                               w0, b0 = model.layers[0].weights\n                               w1, b1 = model.layers[1].weights\n                               return L0(y_true, y_pred) + c1*l1(w0) + c2*l1(w1)\n                           }}})\n\n                  (--*--)\n\n                  #md {{{\n                         Now, we can build a normal model (without regularizers)\n                         but optimized against a regularized loss function.\n                         }}}\n                  (code :python\n                        {{{\n                           model = Sequential([\n                             ...\n                           ])\n\n                           model.compile(loss=regulatd_loss(model), ...)\n                           model.fit(...)\n                           }}})\n                  )\n            )\n\n      (:h2 \"An example\")\n\n      (rows 4 8\n\n            (:h2 \"An unregulated layer\")\n            (image \"unregulated.png\")\n\n            (:h2 \"Regulated layer\")\n            (image \"regulated.png\")\n            )\n      )\n\n(page (:h1 \"Neural networks with categorical output\")\n\n      (rows 4 8\n            (:h2 \"Classification\")\n            #md {{{\n                   - A neural network is aways a function.\n                   - When the function output is a categorical variable, the network\n                   would output $\\mathbf{R}^n$ where $n$ is the number of possible\n                   values of the categorical value.\n                   - This is _classification_.\n                   }}}\n\n            (:h2 \"Probability\")\n            #md {{{\n                   - The network is outputing the probability\n                   of what the _correct_ output should be.\n\n                   - $y_k = p(\\mathrm{prediction} = \\mathrm{value}_k)$\n\n                   - How _certain_ is this classifier?\n                   }}}\n\n            (:h2 \"Entropy\")\n            #md {{{\n                   Entropy is a function on probability distributions:\n\n                   $$ H : \\mathrm{probability\\ distribution} \\to\\mathbf{R} $$\n\n                   defined as:\n\n                   $$H(p) = -\\sum_{u} p(u)\\log p(u)$$\n                   }}}\n            (:h2 \"Intuition\")\n            #md {{{\n                   - $H(p)$ large means $p$ is uncertain\n                   - $H(p)$ small means $p$ is very certain on few outcomes\n                   - $p(u) = 1/n$ gives the largest possible entropy.\n                   }}}\n\n            (:h2 \"Perplexity\")\n            #md {{{\n                   $$\\mathrm{perplex}(p) = 2^{H(p)}$$\n                   }}}\n            )\n\n      #md {{{\n             **Q**: Can we design the loss function to influence the perplexity\n             of a classifier?\n\n             **Q**: Do we want high perplexity or low perplexity?\n             }}}\n      )\n\n(page (:h1 \"Dirichlet Regulation\")\n\n      (rows 4 8\n            (:h2 \"The Dirichlet loss function\")\n            #md {{{\n                   $$L(p) = -c\\cdot (\\alpha - 1)\\cdot \\sum_{u}\\log p(u)$$\n\n                   - for $\\alpha \u003c 1$, $L$ encourages low perplexity (very certain classifiers)\n                   - for $\\alpha \u003e 1$, $L$ encourages high perplexity (uncertain classifiers)\n                   }}}\n            )\n\n      #md {{{\n             **Note**:\n\n             \u003e Here we are talking about a loss function, *not* a regularizer.\n             }}}\n\n      (:h1 \"An example\")\n\n      (rows 4 8\n            (:h2 \"A MNIST classifer\")\n            #md {{{\n                   The model is a two-layer MLP.\n                   }}}\n\n            (:h2 \"No Dirichlet loss\")\n            (image \"no-dirichlet.png\")\n\n            (:h2 \"Dirichlet loss with alpha \u003c 1\")\n            (image \"certain-dirichlet.png\")\n\n            (:h2 \"Dirichlet loss with alpha \u003e 1\")\n            (image \"uncertain-dirichlet.png\")\n\n            (:h2 \"Results\")\n            (:div #md {{{\n                         ```\n                                                   Training (just 1 epoch)\n                         Decisive alpha \u003c 1        83.70%\n                         No dirichlet              85.29%\n                         Indecisive \\alpha \u003e 1     85.32%\n                         ```\n                         }}}\n                  (--*--)\n                  #md {{{\n                         ```\n                                                   Test (just 1 epoch)\n                         Decisive alpha \u003c 1        90.30%\n                         No dirichlet              90.97%\n                         Indecisive \\alpha \u003e 1     91.47%\n                         ```\n                         }}})\n            )\n      )\n", "istop": false, "path": "regulation"}, "representations": {"children": [], "content": "{:title \"Representations\"\n :rank 4\n :summary #md {{{\n                 We will look at how many interesting data structures\n                 can be encoded as tensors.\n                 }}}\n }\n\n(page :wide (file :include \"tensor_representation.html\"))\n", "istop": false, "path": "representations"}, "sequence-learning": {"children": [], "content": "{:title \"Temporal Learning With Memory\"\n :rank 15\n :author #md {{{\n                - Ken Pu\n                - Faculty of Science, UOIT\n                - Copyright, 2017\n                }}}\n :summary #md {{{\n                 When training data comes as a sequence, MLP and CNN fail to capture\n                 the temporal relationship between training samples.  This is when\n                 we need _memory_ as part of the neural network.  \n                 _Recurrence Neural Networks_ (RNN) uses feedback to retain\n                 memory which can be used to model the temporal relationship.\n                 While RNN is effective learn temporal sequences with short memory span,\n                 _Long-short term memory_ (LSTM) networks excel at capturing temporal dependencies\n                 over very long time span.  LSTM is the basis of many state-of-art\n                 natural language models including language-to-language translation.\n                 }}}\n }\n\n(page #md {{{\n             _Confession_:\n\n             \u003e This entire lecture is a rip-off of the outstanding\n             blog article by _Christopher Olah_ which can be found\n             here: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n             }}})\n\n(page (:h1 \"Sequential data\")\n\n      (rows :sep 4 8\n            #md {{{ \n                   What if the training data has a natural order?  \n                   }}}\n            #md {{{\n                   Each sample is a:\n\n                   - a frame in a video sequence,\n                   - a word in a sentence,\n                   - a note in a piece of music\n                   - ...\n                   }}}\n            \n\n            #md {{{\n                   ## Sequence-to-sequence functions\n                   }}}\n            (:div #md {{{\n                         Let the sequence be written as:\n\n                         `$$ s = \\left\u003cx_0, x_1, \\dots\\right\u003e $$`\n\n                         It might be infinite.\n                         }}}\n\n                  #md {{{\n                         Sequence to sequence functions map\n                         each element of one sequence to an\n                         element in the output sequence.}}}\n                  (image \"seq2seq.png\"))\n\n            #md {{{\n                   ## Examples\n\n                   Let\u0027s look at some real-life\n                   of sequence-to-sequence functions\n                   that we would like to learn.\n                   }}}\n            #md {{{\n                   |Input seq | Output seq |\n                   |----------|------------|\n                   | video    | text description per frame |\n                   | text in English | text in French |\n                   | Soundwave | Musical notes |\n                   | Curve in 2D, handwritting | alphabetical symbols |\n                   }}}\n\n\n            (:h2 \"Naive element-to-element mapping\")\n\n            (:div #md {{{\n                         We can naively believe that `$y_i$` \n                         only dependes on `$x_i$`.\n\n                         Namely,\n\n                         `$$y_i = f(x_i)$$`\n                         }}}\n                  #md {{{\n                         This reduces to the traditional learning\n                         of a function $f$ using any of the previously\n                         discussed networks such as MLP.\n                         }}})\n\n\n\n            (:h2 \"Memory base mapping\")\n\n            (:div #md {{{\n                         Must seq-2-seq functions generating `$y_i$`\n                         based on `$x_i$` and `$x_{i-1}$` and `$x_{i-2}$, and so on.\n                         }}}\n                  #md {{{\n                         This can be described by the additional _state_\n                         information that is needed to produce the output.\n                         }}}\n                  (image \"seq2seq-stateful.png\")\n\n                  #md {{{\n                         The function takes state and input, and returns\n                         the output and next state.\n\n                         `$$\n                         \\left[\\begin{array}{c}\n                               y_i \\\\\n                               s_{i+1}\n                               \\end{array}\\right] = f(x_i, s_i)\n                         $$`\n                         }}}\n                  )\n            ); rows\n      ); page\n\n(page (:h1 \"Recurrence Neural Network\")\n\n      #md {{{\n             How can we design a neural network to learn a stateful\n             sequence-to-sequence function as given by:\n             \n             `$$\n             \\left[\\begin{array}{c}\n                   y_i \\\\\n                   s_{i+1}\n                   \\end{array}\\right] = f(x_i, s_i)\n             $$`\n             }}}\n\n      #md {{{\n             Recurence Neural Network (RNN) is the architecture to learn\n             stateful sequence-to-sequence functions.\n             }}}\n\n      (rows :sep 4 8\n\n            (:h2 \"Definition of RNN\")\n            (:div #md {{{\n                         A RNN is defined by:\n\n                         - Its input is a sequence `$\\left\u003c\\mathbf{x}_i: i=0\\dots\\right\u003e$`,\n                         where `$\\mathbf{x}_i\\in\\mathbb{R}^m$`.\n\n                         - There are three weight matrices: \n                            - `$\\mathbf{U}\\sim m\\times k$`\n                            - `$\\mathbf{W}\\sim k\\times k$`\n                            - `$\\mathbf{V}\\sim k\\sim n$`\n\n                         - There are two activation functions:\n                            - `$f$`: mapping input and previous state to current state\n                              (e.g. `relu`)\n                            - `$g$`: mapping current state to current output\n                              (e.g. `softmax`)\n                         }}}\n                  #md {{{\n                         - `$\\mathbf{s}_i = f(\\mathbf{U}\\cdot\\mathbf{x}_i + \\mathbf{W}\\cdot\\mathbf{s}_{i-1})$`\n                         - `$\\mathbf{y}_i = g(\\mathbf{V}\\cdot\\mathbf{s}_i)$`\n                         }}})\n\n\n            (:h2 \"Training of RNN\")\n            (:div #md {{{\n                         The model parameters are:\n\n                         - $\\mathbf{U}$\n                         - $\\mathbf{W}$\n                         - $\\mathbf{V}$\n                         - `$\\mathbf{s}_{-1}$` is the initial state\n                         }}}\n                  #md {{{\n                         The training is done by backpropagation\n                         on the unfolded version of the RNN.\n                         }}}\n                  (image \"rnn-unfolded.jpg\"\n                         #md {{{\n                                Source: http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n                                }}}))\n\n            (:h2 \"Limited Memory of RNN\")\n            (:div #md {{{\n                         RNN uses the state to _remember_ the last input sequence elements.\n\n                         However, the memory is quite limited to a small\n                         window.  Namely `$y_i$` is dependent on `$x_i$`, and loosly on\n                         `$x_{i-1}$`, `$x_{i-2}`, and diminishingly weakly on\n                         `$x_{i-n}, x_{i-n-1}, \\dots$` for larger $n$.\n                         }}}\n                  (image \"rnn-deps.png\"\n                         #md {{{\n                                Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n                                }}}))\n\n\n            (:h2 \"Long-term memory with forgetfulness\")\n            (:div #md {{{ \n                         What if we try to preserve the state in the unfolded RNN?\n                         }}}\n                  (image \"lstm-c-line.png\"\n                         #md {{{Source http://colah.github.io/posts/2015-08-Understanding-LSTMs/}}})\n\n                  #md {{{\n                         _Long-short term memory (LSTM):_\n\n                         \u003e - Old memory is `$C_{t-1}$`, and new memory is `$C_{t+1}$`.\n                         \u003e - The line can preserve memory indefinitely, which is another problem.\n                         \u003e - We will add to the memory line:\n                         \u003e    - a _forgetfulness_, and\n                         \u003e    - addition of new memory.\n                         \u003e - We will also generate outputs from the new memory. \n                         }}}\n                  )\n            ); rows\n      ); page\n\n(page (:h1 \"LSTM\")\n\n      #md {{{\n             An LSTM unit accepts a sequence of $n$ dimensional vectors as input, and\n             outputs a single scalar.  Typically, we would use multiple LSTM units\n             if we want the output to be a vector.\n             }}}\n\n      #md {{{\n             We focus on the case of a _single_ LSTM unit.\n             }}}\n\n      (rows :sep 4 8\n            (:h2 \"Memory line\")\n            #md {{{\n                   - The memory is a single scalar value `$C_{t}$`.\n                   - The output is `$h_{t}$`.\n                   - The input is `$x_{t}$`.\n\n                   Each LSTM computes the mapping:\n\n                   `$$ \\left[\\begin{array}{c}\n                             C_{t-1}\\\\\n                             h_{t-1}\n                             \\end{array}\\right]\n                       \\mapsto\n                       \\left[\\begin{array}{c}\n                             C_{t}\\\\\n                             h_{t}\n                             \\end{array}\\right]$$`\n                   }}}\n\n\n\n            (:h2 \"Forgetfulness\")\n\n            (:div #md {{{\n                         A scalar $f\\in[0, 1]$ is a forgetful factor.\n\n                         It is to be applied to the previous memory:\n                         `$$C_{t-1}\\mapsto f\\cdot C_{t-1}$$`\n\n                         So if $f=0$, we would completely forget the previous\n                         memory.\n                         }}}\n                  (image \"lstm-f.png\"\n                         #md \"Source http://colah.github.io/posts/2015-08-Understanding-LSTMs/\")\n                  )\n\n            (:h2 \"Addition to memory\")\n            (:div #md {{{\n                         We compute the additive memory and its intensity separately.\n                         }}}\n                  #md {{{\n                         New memory:\n\n                         \u003e `$\\tilde C_t = \\mathrm{tanh}(W_2\\cdot[h_{t-1}, x_{t}, 1])$`\n\n                         This provides a chance for LSTM to learn new information based\n                         to be added to the memory line.\n                         }}}\n\n                  #md {{{\n                         Interestingness:\n\n                         \u003e `$i_t = \\sigma(W_3\\cdot[h_{t-1}, x_t, 1])$`\n\n                         This provides a chance for LSTM to determine the interestingness of\n                         the new information `$\\tilde C_t$`.\n                         }}}\n\n                  (image \"lstm-i.png\"\n                         #md \"Source http://colah.github.io/posts/2015-08-Understanding-LSTMs/\")\n                  )\n\n\n            (:h2 \"New memory\")\n            (:div #md {{{\n                         The new memory is computed as:\n\n                         `$$ C_t\\mapsto f\\cdot C_t + i_t\\cdot\\tilde C_t $$`\n                         }}})\n            ); rows\n\n      (row 6 6\n           #md {{{\n                  Finally, the LSTM generates its output based on the new memory `$C_t$`,\n                  the previous output `$h_{t-1}$` and the input `$x_t$`.\n\n                  `$$\n                  h_t = \\sigma(W_4\\cdot [h_{t-1}, x_t, 1])\\cdot \\mathrm{tanh}(C_t)\n                  $$`\n                  }}}\n           (image \"lstm-o.png\" \n                  #md \"Source http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"))\n      ); page\n\n(page (:h1 \"Tensorflow API\")\n\n      #md {{{\n             Keras API provides an implementation of the [LSTM](https://keras.io/layers/recurrent/)\n             layer.\n\n             ```python\n             tf.keras.layers.LSTM\n             ```\n             }}})\n", "istop": false, "path": "sequence-learning"}, "tensorflow": {"children": [], "content": "{:title \"Introduction to Tensorflow\"\n :rank 7\n :summary #md {{{\n                 Tensorflow is able to handle varying tensors using\n                 _variables_.  The manipulation of variables is done\n                 by a collection of APIs based on the principles of Calculus.\n                 }}}\n }\n\n(page (file :include \"tensorflow.html\"))\n", "istop": false, "path": "tensorflow"}, "tensorflow-regression": {"children": [], "content": "{:title \"Regression models in Tensorflow\"\n :rank 8\n :summary #md {{{\n                 We implement linear and logistic regression models\n                 in Tensorflow.  The emphasis is to highlight\n                 a generic framework that can unify the two\n                 regression models into a common API.\n                 }}}\n }\n\n(page :wide\n      (file :include \"regression_models.html\"))\n", "istop": false, "path": "tensorflow-regression"}, "tensors": {"children": [], "content": "{:title \"Tensors\"\n :rank 2\n :summary #md {{{\n                 Tensors are the universal building block of neural computation.\n                 We will provide the definition, basic properties and operations of tensors, and demonstrate\n                 their expressive power.\n                 }}}\n }\n\n(:h1 \"Tensors and some definitions\")\n(page (:h1 \"Vectors\")\n\n      (rows :sep 4 8\n\n            (:h2 \"Vector as a list\")\n            (:div #md {{{\n                         A **vector** is a _list_ of numerical values.\n\n                         _Example_\n\n                         \u003e $v = [1.0, 2.0]$\n                         }}}\n                  #md {{{\n                         **Dimensionality**\n\n                         The length of the vector is called the dimensionality\n                         of the vector.\n\n                         \u003e - $v = [1.0, 2.0]$\n                         \u003e - $\\mathrm{len}(v) = 2$, and $\\mathrm{dim}(v) = 2$\n                         \u003e - We can also write it as: $v\\in\\mathbf{R}^2$\n                         }}}\n                  #md {{{\n                         **Indices**\n\n                         We can refer to the individual entries in a vector\n                         by their positions.\n\n                         \u003e With $v = [1.0, 2.0]$, we can write:\n                         \u003e\n                         \u003e - $v[0] = 1.0$\n                         \u003e - $v[1] = 2.0$\n                         }}}\n                  )\n\n            (:h2 \"Vector as a multi-dimensional array\")\n            #md {{{\n                   **Axes and rank**\n\n                   To specify a specific entry of a vector, we need\n                   *one* index $i\\in[0\\dots\\mathrm{dim}(v)-1]$.\n\n                   - So, we say that *vectors* have **one** axis.\n\n                   - Also, we say that the rank of vectors is **one**.\n                   }}}\n\n            (:h2 \"The shape\")\n            #md {{{\n                   The shape of a vector is a tuple indicating the size\n                   of its axis.\n\n                   - $v = [1.0, 2.0]$\n                   - $\\mathrm{shape}(v) = (2,)$\n                   }}}\n            )\n      )\n\n(page (:h1 \"Matrices\")\n\n      (rows :sep 4 8\n            (:h2 \"Matrices as nested list\")\n            (:div #md {{{\n                         A matrix is a 2D-array.\n\n                         **Example**\n\n                         ```\n                         $$ W = \\left[\\begin{array}{ccc}\n                                      1.0 \u0026 2.0 \u0026 3.0 \\\\\n                                      4.0 \u0026 5.0 \u0026 6.0 \\\\\n                                      7.0 \u0026 8.0 \u0026 9.0 \\\\\n                                      \\end{array}\n                                      \\right] $$\n                         ```\n                         }}}\n                  #md {{{\n                         We need **two** indices address the entries:\n\n                         - row index\n                         - column index\n\n                         \u003e We have:\n                         \u003e\n                         \u003e - $W[0, 1] = 2.0$\n                         \u003e - $W[2, 1] = 8.0$\n                         }}})\n            (:h2 \"Axes, rank and shapes\")\n            (:div #md {{{\n                         **Axes and rank** of matrices\n\n                         - A matrix has two axes because it needs two indices\n                         to specify a unique entry.\n\n                         - The rank of matrices is **two**.\n                         }}}\n                  #md {{{\n                         **Shape**\n\n                         - With \n                         \n                         ```\n                         $$W = \\left[\\begin{array}{ccc}\n                                           1 \u0026 2 \u0026 3 \\\\\n                                           4 \u0026 5 \u0026 6.2 \\\\\n                                           \\end{array}\\right]$$\n                         ```\n                         - $\\mathrm{shape}(W) = (2, 3)$\n                         }}}\n                  )\n            )\n      )\n\n(page (:h1 \"Higher rank tensors\")\n\n      #md {{{\n             We can know generate _vectors_ and _matrices_ to __tensors__.\n             }}}\n\n      (rows :sep 4 8\n            (:h2 \"Many axes\")\n            #md {{{\n                   A tensor can have many axes.  Consider\n                   a tensor $x$ with $\\mathrm{rank}(x) = 3$.\n\n                   This means that we need __three__ indices to uniquely specify\n                   an entry in $x$:\n\n                   $$ x[i,j,k] $$\n                   }}}\n\n            (:h2 \"Shape\")\n            (:div #md {{{\n                         A tensor can have any tuple (of positive integers) as a shape.\n\n                         *Example*\n\n                         \u003e Consider the tensor $x$ having shape\n                         \u003e ```\n                         \u003e $$\n                         \u003e \\mathrm{shape}(x) = (2, 2, 3)\n                         \u003e $$\n                         \u003e ```\n\n                         An example of $x$ would be:\n\n                         ```\n                         $$\n                         x = \\left[\\begin{array}{c}\n                                   \\left[\\begin{array}{ccc}\n                                         1 \u0026 2 \u0026 3 \\\\\n                                         4 \u0026 5 \u0026 6 \\\\\n                                         \\end{array}\\right] \\\\\n                                   \\\\\n                                   \\left[\\begin{array}{ccc}\n                                         7 \u0026 8 \u0026 9 \\\\\n                                         10 \u0026 11 \u0026 12 \\\\\n                                         \\end{array}\\right] \\\\\n                                   \\end{array}\\right]\n                         $$\n                         ```\n                         }}}\n                  #md {{{\n                         How do we address the value `12`?\n\n                         The second _block_ along the _first axis_ $\\Rightarrow$  $i = 1$.\n\n                          \u003e ```\n                          \u003e $\n                          \u003e \\left[\\begin{array}{ccc}\n                          \u003e  7 \u0026 8 \u0026 9 \\\\\n                          \u003e  10 \u0026 11 \u0026 \\color{red}{12} \\\\\n                          \u003e  \\end{array}\\right]\n                          \u003e $\n                          \u003e ``` \n\n                          The second row in that block $\\Rightarrow$ $j = 1$.\n\n                          \u003e $ [\\begin{array}{ccc}10 \u0026 11 \u0026 \\color{red}{12}\\end{array}] $\n\n                          The third column in that row $\\Rightarrow$ $k=2$\n\n                          \u003e $\\color{red}{12}$\n\n                          $$ x[1, 1, 2] = 12 $$\n                            }}}\n                  )\n            )\n      )\n\n(page (:h1 \"Scalars\")\n\n      #md {{{\n             A curious fact about scalars.  They are also tensors.\n             }}}\n\n      #md {{{\n             How many indices do we need to specify a scalar from a scalar.\n             }}}\n\n      (box #md {{{\n                  Zero\n                  }}})\n\n      #md {{{\n             - Scalars are tensors\n             - They have zero axes, so $\\mathrm{rank}(\\mathrm{scalar}) = 0$.\n             - Their shapes are the _empty tuple_: `()`\n             }}}\n      )\n\n(:h1 \"Functions over tensors\")\n\n(page (:h1 \"General functions\")\n\n      (rows :sep 4 8\n\n            (:h2 \"Representing all possible tensors\")\n            (:div #md {{{\n                         **Scalar**\n\n                         - We are interested in real numbers.  The set of _all_ real numbers is denoted by $\\mathbf{R}$.\n                         - The set of _all_ integers numbers (positive and negative) is denoted by $\\mathbf{N}$.\n                         - The set of _all_ natural numbers ($\\geq 0$) is denoted by $\\mathbf{N}^+$.\n                         }}}\n\n                  #md {{{\n                         **Vectors**\n\n                         - The set of _all_ pairs of real numbers is written as $\\mathbf{R}\\times \\mathbf{R}$.\n                         - We can _also_ write set of pairs as a power: $\\mathbf{R}^2$.\n                         - More generally we can extend the notation to arbitrary tuples of reals: $\\mathbf{R}^k$.\n                         }}}\n\n                  #md {{{\n                         _Example_\n\n                         \u003e - $3.14 \\in\\mathrm{R}$\n                         \u003e - $[1.5, -0.5] \\in \\mathrm{R}^2$, or we can also say $[1.5, -0.5]\\in\\mathrm{R}\\times\\mathrm{R}$.\n                         }}}\n\n                  #md {{{\n                         **Matrices**\n\n                         A matrix with shape $(m, n)$ has $m\\times n$ number of entries.  The set of all matrices with\n                         the shape $(m, n)$ is written:\n\n                         $$ \\mathrm{R}^{m\\times n}$$\n                         }}}\n                  (note #md {{{\n                               Note, $\\mathrm{R}^{m\\cdot n}$ is the set of vectors with dimensionality of $m\\cdot n$.\n\n                               So, $\\mathrm{R}^{mn}\\not=\\mathrm{R}^{m\\times n}$.\n                               }}})\n                  #md {{{\n                         **Tensors**\n\n                         The set of all possible tensors with shape $(m,n,k)$ is written:\n                         $$ \\mathrm{R}^{m\\times n\\times k} $$\n                         }}}\n                  )\n\n            (:h2 \"Functions\")\n            (:div #md {{{\n                         A function takes inputs and returns an output.  We express the _functional signature_ as:\n\n                         $$ f : \\mathrm{Input} \\to \\mathrm{Output} $$\n                         }}}\n                  #md {{{\n                         Here are some examples of functional signatures:\n\n                         - $f: \\mathbf{R} \\to \\mathbf{R} $\n                         - $f: \\mathbf{R}^2 \\to \\mathbf{R} $\n                         - $f: \\mathbf{R}^2 \\times \\mathbf{R}^2 \\to \\mathbf{R}$\n                         - $f: \\mathbf{R}^{3\\times 4} \\times\\mathrm{R}^{4\\times 2} \\to \\mathrm{R}^{3\\times 2} $\n                         - $f: \\to \\mathrm{R}$\n                         - $f: \\mathrm{R} \\to \\mathrm{R}^{3\\times 3}$\n                         }}}\n                  )\n            )\n      )\n\n(page (:h1 \"Elementary functions\")\n\n      #md {{{\n             Let\u0027s look at some specific functions.\n             }}}\n\n      (rows 4 8\n            (:h2 \"Element-wise operations\")\n            (:div #md {{{\n                         We can combine tensors with basic arithmetics.\n                         }}}\n                  #md {{{\n                         $$+ : \\mathrm{R}^{m\\times n\\times k} \\times \\mathrm{R}^{m\\times n\\times k} \\to \\mathrm{R}^{m\\times n\\times k}$$\n\n                         where given $x, y \\in \\mathrm{R}^{m\\times n\\times k}$\n\n                         $$(x + y)[i, j, k] = x[i, j, k] + y[i, j, k]$$\n                         }}}\n                  )\n\n            (:h2 \"Transpose\")\n            (:div #md {{{\n                         Matrix transpose:\n\n                         $T : \\mathbf{R}^{m\\times n} \\to\\mathbf{R}^{n\\times m}$\n\n                         where given $x\\in\\mathbf{R}^{m\\times n}$,\n\n                         $$ x^T[i, j] = x[j, i] $$\n                         }}}\n                  #md {{{\n                         Tensor transpose:\n\n                         We can generalize matrix transpose to higher ranks.\n\n                         $$\\mathrm{transpose}(\\circ, \\pi) : \\mathbf{R}^{m\\times n\\times k} \\to \\mathbf{R}^{\\pi(m)\\times \\pi(n)\\times \\pi(k)}$$\n\n                         where $\\pi:\\mathbf{N}^+ \\to\\mathbf{N}^+$ is a _permutation_.\n                         }}}\n                  #md {{{\n                         $$ x^T = \\mathrm{transpose}(x, (1, 0)) $$\n                         }}}\n                  )\n\n            (:h2 \"Inner product\")\n            (:div #md {{{\n                         $$\\left\u003c\\circ, \\circ\\right\u003e : \\mathbf{R}^n \\times \\mathbf{R}^n \\to \\mathrm{R}$$\n\n                         where\n\n                         ```\n                         $$\\left\u003cx, y\\right\u003e = \\sum_{i=0}^{n-1} x[i]\\cdot y[i]$$\n                         ```\n                         }}})\n\n            (:h2 \"Slicing\")\n            (:div #md {{{\n                         $$ \\mathrm{row}: \\mathbf{R}^{m, n} \\times \\mathbf{N}^+ \\to \\mathbf{R}^n $$\n                         $$ \\mathrm{col}: \\mathbf{R}^{m, n} \\times \\mathbf{N}^+ \\to \\mathbf{R}^m $$\n\n                         $$ x[i,:] = \\mathrm{row}(x, i) $$\n                         $$ x[:, j] = \\mathrm{col}(x, j) $$\n                         }}}\n                  )\n\n            (:h2 \"Matrix multiplication\")\n\n            #md {{{\n                   $$ \\circ : \\mathbf{R}^{m\\times n} \\times\\mathbf{R}^{n\\times p} \\to \\mathbf{R}^{m\\times p} $$\n\n                   where\n\n                   ```\n                   $$\n                   (x\\cdot y)[i, k] = \\left\u003cx[i,:], y[:, k]\\right\u003e\n                   $$\n                   ```\n                   }}}\n            )\n      )\n", "istop": false, "path": "tensors"}, "vector-spaces": {"children": [], "content": "{:title \"Basic Linear Algebra\"\n :summary #md {{{\n                 We will present some definitions\n                 and basic calculations in linear algebra.\n                 }}}\n :rank 3}\n\n(page :wide\n      (iframe \n        \"2019_09_15_linear_algebra.html\"))\n", "istop": false, "path": "vector-spaces"}};
    var root_path = "db.science.uoit.ca/library/machine-learning/";
    var reload_path = window.origin + root_path;
    console.log("app.core.main()");
    app.core.main();
</script>
</body>

<!-- Mirrored from db.science.uoit.ca/library/teaching/machine-learning by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 06 Apr 2020 22:41:39 GMT -->
</html>